{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction to Message Passing Interface (MPI)</center>\n",
    "### <center> Linh B. Ngo </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center>Message Passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Processes communicate via messages\n",
    "- Messages can be\n",
    "    - Raw data to be used in actual calculations\n",
    "    - Signals and acknowledgements for the receiving processes regarding the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center>History of MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Early 80s:**\n",
    "- Various message passing environments were developed\n",
    "- Many similar fundamental concepts\n",
    "- N-cube (Caltech), P4 (Argonne), PICL and PVM (Oakridge), LAM (Ohio SC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** 1992: **\n",
    "- More than 80 reseachers from different institutions in US and Europe agreed to develop and implement a common standard for message passing\n",
    "- First meeting colocated with Supercomputing 1992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** After finalization: **\n",
    "- MPI becomes the *de-factor* standard for distributed memory parallel programming\n",
    "- Available on every popular operating system and architecture\n",
    "- Interconnect manufacturers commonly provide MPI implementations optimized for their hardware\n",
    "- MPI standard defines interfaces for C, C++, and Fortran\n",
    "    - Language bindings available for many popular languages (quality varies)\n",
    "    - MPI4PY: Bindings for python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** 1994: MPI-1 **\n",
    "- Communicators\n",
    "    - Information about the runtime environments\n",
    "    - Creation of customized topologies\n",
    "- Point-to-point communication\n",
    "    - Send and receive messages\n",
    "    - Blocking and non-blocking variations\n",
    "- Collectives\n",
    "    - Broadcast and reduce\n",
    "    - Gather and scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** 1998: MPI-2 **\n",
    "- One-sided communication (non-blocking)\n",
    "    - Get & Put (remote memory access)\n",
    "- Dynamic process management\n",
    "    - Spawn\n",
    "- Parallel I/O\n",
    "    - Multiple readers and writers for a single file\n",
    "    - Requires file-system level support (LustreFS, PVFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** 2012: MPI-3 **\n",
    "- Revised remote-memory access semantic\n",
    "- Fault tolerance model\n",
    "- Non-blocking collective communication\n",
    "- Access to internal variables, states, and counters for performance evaluation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Set up MPI on Palmetto for C/C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Interactive mode:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If use command line terminal:\n",
    "\n",
    "`qsub -I -l select=1:ncpus=8:mpiprocs=8:mem=10gb,walltime=01:00:00`\n",
    "\n",
    "- How to get a JupyterHub interface with the above resources?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`module load gcc/5.3.0 openmpi/1.10.3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*The module load command can be added to a script, which then is to be sourced from inside .bashrc to automate module loading. Calling the module load directly from inside .bashrc is not recommended.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Create a file named **hello.c** inside directory with the following content\n",
    "```\n",
    "#include <stdio.h>\n",
    "#include <sys/utsname.h>\n",
    "#include <mpi.h>\n",
    "int main(int argc, char *argv[]){\n",
    "    MPI_Init(&argc, &argv);\n",
    "    struct utsname uts;\n",
    "    uname (&uts);\n",
    "    printf(\"My process is on node %s.\\n\", uts.nodename);\n",
    "\tMPI_Finalize();\n",
    "\treturn 0;\n",
    "}\n",
    "```\n",
    "- This can be done via JupyterHub's browser-based text editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Compile hello.c\n",
    "```\n",
    "mpicc hello.c -o hello\n",
    "```\n",
    "- Run hello.c\n",
    "```\n",
    "mpirun -np 2 ./hello\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Set up MPI on Palmetto for Python (Interactive via Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Before launching JupyterHub**\n",
    "- Make sure that you have the command ``module load gcc/5.3.0 openmpi/1.10.3`` in your .jhubrc file. If you are using JupyterHub to edit the file, the server will need to be stopped and started again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Before launching a Jupyter notebook (only need to be done once)**\n",
    "- Install mpi4py by executing ``pip install --user mpi4py`` from a terminal. This needs to be done prior to launching a Jupyter notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> The working of MPI in a nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- All processes are launched at the beginning of the program execution\n",
    "    - The number of processes are user-speficied\n",
    "    - Typically, this number is matched to the total number of cores available across the entire cluster\n",
    "- All processes have their own memory space and have access to the same source codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Basic parameters available to individual processes: **\n",
    "```\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "name = MPI.Get_processor_name()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- MPI defines **communicator** groups for point-to-point and collective communications\n",
    "    - Unique IDs (**rank**) are defined for individual processes within a communicator group\n",
    "    - Communications are performed based on these IDs\n",
    "    - Default **global communication** (COMM_WORLD) contains all processes\n",
    "    - For $N$ processes, ranks go from $0$ to $N-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile codes/mpi4py/hello.py\n",
    "#!/usr/bin/env python\n",
    "# hello.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "name = MPI.Get_processor_name()\n",
    "print (\"Hello world from process %s running on host %s out of %s processes\" % \n",
    "       (rank, name, size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world from process 4 running on host node0106.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 5 running on host node0106.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 6 running on host node0106.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 7 running on host node0106.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 0 running on host node0106.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 1 running on host node0106.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 2 running on host node0106.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 3 running on host node0106.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 8 running on host node1171.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 9 running on host node1171.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 10 running on host node1171.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 11 running on host node1171.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 12 running on host node1171.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 13 running on host node1171.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 14 running on host node1171.palmetto.clemson.edu out of 16 processes\r\n",
      "Hello world from process 15 running on host node1171.palmetto.clemson.edu out of 16 processes\r\n"
     ]
    }
   ],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; \\\n",
    "mpirun -np 16 --mca mpi_cuda_support 0 codes/mpi4py/hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ranks are used to enforce execution/exclusion of code segments within the original source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile codes/mpi4py/evenodd.py\n",
    "#!/usr/bin/env python\n",
    "# evenodd.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "name = MPI.Get_processor_name()\n",
    "if (rank % 2 == 0):\n",
    "    print (\"Process %s is even\" % (rank))\n",
    "else:\n",
    "    print (\"Process %s is odd\" % (rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; \\\n",
    "mpirun -np 4 codes/mpi4py/evenodd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ranks can be used as mean to calculate and distributed workload (data) among the processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile codes/mpi4py/workdist.py\n",
    "#!/usr/bin/env python\n",
    "# workdist.py\n",
    "from mpi4py import MPI\n",
    "import random\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "name = MPI.Get_processor_name()\n",
    "A = [2,13,4,3,5,1,0,12,10,8,7,9,11,6,15,14]\n",
    "#print (\"Elements %s and %s are assigned to process %s\" \n",
    "#       % (A[rank%15], A[1+rank%15], rank))\n",
    "if (rank < 4):\n",
    "    print (\"Process %s has elements %s\" % (rank, A[(4*rank):(4*rank+4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/workdist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Individual processes rely on communication (message passing) to enforce workflow\n",
    "    - Point-to-point Communication\n",
    "    - Collective Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Point-to-Point: Send and Receive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "comm = MPI.COMM_WORLD\n",
    "```\n",
    "- Sender process:\n",
    "```\n",
    "comm.send(data, dest_rank)\n",
    "```\n",
    "- Receiver process:   \n",
    "```\n",
    "data = comm.recv(source_rank)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Original MPI C Syntax: MPI_Send**\n",
    "```\n",
    "int MPI_Send(void *buf, \n",
    "\tint count, \n",
    "\tMPI_Datatype datatype, \n",
    "\tint dest, \n",
    "\tint tag, \n",
    "\tMPI_Comm comm)\n",
    "```\n",
    "\n",
    "- MPI_Datatype may be MPI_BYTE, MPI_PACKED, MPI_CHAR, MPI_SHORT, MPI_INT, MPI_LONG, MPI_FLOAT, MPI_DOUBLE, MPI_LONG_DOUBLE, MPI_UNSIGNED_CHAR\n",
    "- *dest* is the rank of the process the message is sent to\n",
    "- *tag* is an integer identify the message. Programmer is responsible for managing tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Original MPI C Syntax: MPI_Recv**\n",
    "```\n",
    "int MPI_Recv(\n",
    "\tvoid *buf, \n",
    "\tint count, \n",
    "\tMPI_Datatype datatype, \n",
    "\tint source, \n",
    "\tint tag, \n",
    "\tMPI_Comm comm,\n",
    "\tMPI_Status *status)\n",
    "```\n",
    "\n",
    "- MPI_Datatype may be MPI_BYTE, MPI_PACKED, MPI_CHAR, MPI_SHORT, MPI_INT, MPI_LONG, MPI_FLOAT, MPI_DOUBLE, MPI_LONG_DOUBLE, MPI_UNSIGNED_CHAR\n",
    "- *source* is the rank of the process from which the message was sent.\n",
    "- *tag* is an integer identify the message. MPI_Recv will only place data in the buffer if the tag from MPI_Send matches. The constant MPI_ANY_TAG may be used when the source tag is unknown or not important. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile codes/mpi4py/sendrecv.py\n",
    "#!/usr/bin/env python\n",
    "# sendrecv.py\n",
    "from mpi4py import MPI\n",
    "import random\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "if (rank == 0):\n",
    "    send_pkg = random.random()\n",
    "    print (send_pkg)\n",
    "    comm.send(send_pkg, dest = 1, tag = 1)\n",
    "if (rank == 1):\n",
    "    recv_pkg = 0\n",
    "    recv_pkg = comm.recv(source = 0, tag = 1)\n",
    "    print (recv_pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/sendrecv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Blocking risks**\n",
    "- Send data larger than available network buffer (Blocking send)\n",
    "- Lost data (or missing sender) leading to receiver hanging indefinitely (Blocking receive)\n",
    "\n",
    "**Data types**\n",
    "- MPI4PY supports all default MPI's data types\n",
    "- MPI4PY uses *pickle* to facilitate sending and receiving of complex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile codes/mpi4py/sendrecv2.py\n",
    "#!/usr/bin/env python\n",
    "# sendrecv2.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "if rank == 0:\n",
    "    data = {'class': 'cpsc3620', 'semester': 'Spring 2017', \n",
    "            'enrollments': 40}\n",
    "    comm.send(data, dest=1, tag=11)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0, tag=11)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/sendrecv2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile codes/mpi4py/sendrecv3.py\n",
    "#!/usr/bin/env python\n",
    "# sendrecv3.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "if rank == 0:\n",
    "    data = [1,2,3,4]\n",
    "    comm.send(data, dest=1, tag=11)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0, tag=11)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/sendrecv3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Collective Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Must involve ALL processes within the scope of a communicator\n",
    "- Unexpected behavior, including programming failure, if even one process does not participate\n",
    "- Types of collective communications:\n",
    "    - Synchronization: barrier\n",
    "    - Data movement: broadcast, scatter/gather\n",
    "    - Collective computation (aggregate data to perform computation): Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/05/mpi-collective.png\" width=\"700\"/> \n",
    "<sub> *https://computing.llnl.gov/tutorials/mpi/* </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**broadcast:**\n",
    "```\n",
    "comm = MPI.COMM_WORLD\n",
    "```\n",
    "- All processes:\n",
    "```\n",
    "<buffer at receiving process> = comm.bcast(<original data>, root=<root process>)\n",
    "```\n",
    "- *root* process is the one that has the original data initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "int MPI_Bcast(\n",
    "\tvoid *buf, \n",
    "\tint count, \n",
    "\tMPI_Datatype datatype, \n",
    "\tint root, \n",
    "\tMPI_Comm comm);\n",
    "```\n",
    "- Donâ€™t need to specify a TAG or DESTINATION\n",
    "- Must specify the SENDER (root)\n",
    "- Blocking call for all processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/bcast.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/bcast.py\n",
    "#!/usr/bin/env python\n",
    "# bcast.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD; rank = comm.Get_rank();\n",
    "if rank == 0:\n",
    "    data = 2\n",
    "elif (rank != 1):\n",
    "    data = -1\n",
    "else:\n",
    "    data = -2\n",
    "print (\"%s: %s\" % (rank, data))\n",
    "if (rank != 1):\n",
    "    data = comm.bcast(data, root=0)\n",
    "    print (\"%s: %s\" % (rank, data))\n",
    "else:\n",
    "    data1 = comm.bcast(data, root=0)\n",
    "    print (\"%s: %s\" % (rank, data))\n",
    "if (rank == 1):\n",
    "    print(\"%s: %s\" % (rank, data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: -2\r\n",
      "1: -2\r\n",
      "1: 2\r\n",
      "2: -1\r\n",
      "3: -1\r\n",
      "3: 2\r\n",
      "0: 2\r\n",
      "0: 2\r\n",
      "2: 2\r\n"
     ]
    }
   ],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/bcast.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/bcast2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/bcast2.py\n",
    "#!/usr/bin/env python\n",
    "# bcast2.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'key1' : [7, 2.72, 2+3j],\n",
    "            'key2' : ( 'abc', 'xyz')}\n",
    "else:\n",
    "    data = None\n",
    "data = comm.bcast(data, root=0)\n",
    "print (\"process %s\" % (rank))\n",
    "print (rank,data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 3\r\n",
      "3 {'key1': [7, 2.72, (2+3j)], 'key2': ('abc', 'xyz')}\r\n",
      "process 0\r\n",
      "0 {'key2': ('abc', 'xyz'), 'key1': [7, 2.72, (2+3j)]}\r\n",
      "process 1\r\n",
      "1 {'key2': ('abc', 'xyz'), 'key1': [7, 2.72, (2+3j)]}\r\n",
      "process 2\r\n",
      "2 {'key1': [7, 2.72, (2+3j)], 'key2': ('abc', 'xyz')}\r\n"
     ]
    }
   ],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/bcast2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**scatter:**\n",
    "```\n",
    "comm = MPI.COMM_WORLD\n",
    "```\n",
    "- All processes:\n",
    "```\n",
    "<buffer at receiving process> = comm.scatter(<original array>, root=<root process>)\n",
    "```\n",
    "- *root* process is the one that has the original data array initially. \n",
    "- Data are divided according to rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Original MPI C Syntax: MPI_Scatter**\n",
    "```\n",
    "int MPI_Scatter(\n",
    "\tvoid *sendbuf, \n",
    "\tint sendcount, \n",
    "\tMPI_Datatype sendtype, \n",
    "\tvoid *recvbuf,\n",
    "\tint recvcnt,\n",
    "\tMPI_Datatype recvtype,\n",
    "\tint root, \n",
    "\tMPI_Comm comm);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/scatter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/scatter.py\n",
    "#!/usr/bin/env python\n",
    "# scatter.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size();rank = comm.Get_rank(); print(rank)\n",
    "\n",
    "if rank == 0:\n",
    "    data = [(i+1)**2 for i in range(size)]\n",
    "    print (data)\n",
    "else:\n",
    "    data = None\n",
    "partial_data = comm.scatter(data, root=0)\n",
    "print (data)\n",
    "print (rank, partial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "[1, 4, 9, 16]\r\n",
      "[1, 4, 9, 16]\r\n",
      "0 1\r\n",
      "1\r\n",
      "None\r\n",
      "1 4\r\n",
      "2\r\n",
      "None\r\n",
      "2 9\r\n",
      "3\r\n",
      "None\r\n",
      "3 16\r\n"
     ]
    }
   ],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/scatter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/scatter2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/scatter2.py\n",
    "#!/usr/bin/env python\n",
    "# scatter2.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = [{'key1' : [7, 2.72, 2+3j]},\n",
    "            {'key2' : ( 'abc', 'xyz')},\n",
    "            {'key3' : ( 'abc', 'xyz')},\n",
    "            {'key4' : ( 'cde', 'xyz')}]\n",
    "else:\n",
    "    data = None\n",
    "data = comm.scatter(data, root=0)\n",
    "print (\"%s: %s\" % (rank, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: {'key1': [7, 2.72, (2+3j)]}\r\n",
      "1: {'key2': ('abc', 'xyz')}\r\n",
      "2: {'key3': ('abc', 'xyz')}\r\n",
      "3: {'key4': ('cde', 'xyz')}\r\n"
     ]
    }
   ],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/scatter2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**gather:**\n",
    "```\n",
    "comm = MPI.COMM_WORLD\n",
    "```\n",
    "- All processes:\n",
    "```\n",
    "<buffer at sending process> = comm.gather(<final array>, root=<root process>)\n",
    "```\n",
    "- *root* process is the one that receives the original data array initially. \n",
    "- Data arrive and are sorted at *root* according to rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Original MPI C Syntax: MPI_Gather**\n",
    "```\n",
    "int MPI_Gather(\n",
    "\tvoid *sendbuff, \n",
    "\tint sendcount, \n",
    "\tMPI_Datatype sendtype, \n",
    "\tvoid *recvbuff,\n",
    "\tint recvcnt,\n",
    "\tMPI_Datatype recvtype,\n",
    "\tint root, \n",
    "\tMPI_Comm comm);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/gather.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/gather.py\n",
    "#!/usr/bin/env python\n",
    "# gather.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "data = (rank+1)**2\n",
    "print (\"%s: %s\" % (rank, data))\n",
    "all_data = comm.gather(data, root=0)\n",
    "if rank == 0:\n",
    "    print (\"%s: %s\" % (rank, data))\n",
    "    print (all_data)\n",
    "else:\n",
    "    print (\"%s: %s\" % (rank, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: 16\r\n",
      "3: 16\r\n",
      "0: 1\r\n",
      "0: 1\r\n",
      "[1, 4, 9, 16]\r\n",
      "1: 4\r\n",
      "1: 4\r\n",
      "2: 9\r\n",
      "2: 9\r\n"
     ]
    }
   ],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**reduce**\n",
    "```\n",
    "comm = MPI.COMM_WORLD\n",
    "```\n",
    "- All processes:\n",
    "```\n",
    "<final result at sending process> = comm.reduce(<data to be reduced>, op=MPI.<operation>, root=<root process>)\n",
    "```\n",
    "- *root* process is the one that receives the final reduced data initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Original MPI C Syntax: MPI_Reduce**\n",
    "```\n",
    "int MPI_Reduce(\n",
    "\tvoid *sendbuf, \n",
    "\tvoid *recvbuff,\n",
    "\tint count, \n",
    "\tMPI_Datatype datatype, \n",
    "\tMPI_OP op,\n",
    "\tint root, \n",
    "\tMPI_Comm comm);\n",
    "```\n",
    "- MPI_Op may be MPI_MIN, MPI_MAX, MPI_SUM, MPI_PROD (twelve total)\n",
    "- Programmer may add operations, must be commutative and associative\n",
    "- If count > 1, then operation is performed element-wise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/reduce.py\n",
    "#!/usr/bin/env python\n",
    "# reduce.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank() \n",
    "\n",
    "sum = comm.reduce(rank, op=MPI.SUM, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print (\"The reduction is %s\" % (sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reduction is 10\r\n"
     ]
    }
   ],
   "source": [
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 5 codes/mpi4py/reduce.py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
