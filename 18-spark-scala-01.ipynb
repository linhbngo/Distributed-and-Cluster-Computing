{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Spark In-memmory Computing via Spark Scala </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark is an implementation of the MapReduce programming paradigm that operates on in-memory data and allows data reuses across multiple computations.\n",
    "- Performance of Spark is significantly better than its predecessor, Hadoop MapReduce. \n",
    "- Spark's primary data abstraction is Resilient Distributed Dataset (RDD):\n",
    "    - Read-only, partitioned collection of records\n",
    "    - Created (aka written) through deterministic operations on data:\n",
    "        - Loading from stable storage\n",
    "        - Transforming from other RDDs\n",
    "        - Generating through coarse-grained operations such as map, join, filter ...\n",
    "    - Do not need to be materialized at all time and are recoverable via **data lineage**\n",
    "\n",
    "<img src=\"pictures/18/spark2_arch.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSH into CloudLab. \n",
    "\n",
    "```\n",
    "$ ssh clnode218.clemson.cloudlab.us\n",
    "```\n",
    "\n",
    "From inside the terminal, open Spark's interactive shell\n",
    "\n",
    "```\n",
    "$ spark-shell --master yarn --driver-memory 1G --executor-memory 10G --num-executors 10 --verbose --conf \"spark.port.maxRetries=40\"\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "Spark stores data in memory. This memory space is represented by variable **sc** (SparkContext). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spark shell's prompt is **scala>**\n",
    "\n",
    "```\n",
    "scala> sc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "val textFile = sc.textFile(\"hdfs:///repository/gutenberg-shakespeare.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type is `textFile`?\n",
    "\n",
    "```\n",
    "scala> print (textFile)\n",
    "scala> printf (textFile)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What does Spark do with my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storage Level:**\n",
    "- Does RDD use disk?\n",
    "- Does RDD use memory?\n",
    "- Does RDD use off-heap memory?\n",
    "- Should an RDD be serialized (while persisting)?\n",
    "- How many replicas (default: 1) to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "scala> textFile.getStorageLevel.useDisk\n",
    "scala> textFile.getStorageLevel.useMemory\n",
    "scala> textFile.getStorageLevel.useOffHeap\n",
    "scala> textFile.getStorageLevel.deserialized\n",
    "scala> textFile.getStorageLevel.replication\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cache to Spark:\n",
    "\n",
    "```\n",
    "scala> textFile.cache\n",
    "scala> textFile.getStorageLevel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, each transformed RDD may be recomputed each time you run an action on it.\n",
    "- It is also possible to *persist* RDD in memory using *persist()* or *cache()*\n",
    "    - *persist()* allows you to specify level of storage for RDD\n",
    "    - *cache()* only persists RDD in memory\n",
    "    - To retire RDD from memory, *unpersist()* is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data operations in Spark are categorized into two groups, *transformation* and *action*. \n",
    "- A *transformation* creates new dataset from existing data. Examples of *transformation* include map, filter, reduceByKey, and sort. \n",
    "- An *action* returns a value to the driver program (aka memory space of this notebook) after running a computation on the data set. Examples of *action* include count, collect, reduce, and save. \n",
    "\n",
    "\"All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program.\" -- Spark Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD Operations in Spark\n",
    "\n",
    "**Transformations: **\n",
    "\n",
    "- *map*(f: T -> U) : RDD[T] -> RDD[U]\n",
    "- *filter*(f: T -> Bool) : RDD[T] -> RDD[T]\n",
    "- *flatMap*(f: T -> Seq[U]) : RDD[T] -> RDD[U]\n",
    "- *sample*(*fraction*: Float) : RDD[T] -> RDD[T] (deterministic sampling)\n",
    "- *groupByKey*() : RDD[(K,V)] -> RDD[(K, Seq[V])]\n",
    "- *reduceByKey*(f: (V,V) -> V) : RDD[(K,V)] -> RDD[(K,V)]\n",
    "- *union*() : (RDD[T], RDD[T]) -> RDD[T]\n",
    "- *join*() : (RDD[(K,V)], RDD[(K,W)]) -> RDD[(K,(V,W))]\n",
    "- *cogroup*() : (RDD[(K,V)], RDD[(K,W)] -> RDD[(K, (Seq[V],Seq[W]))]\n",
    "- *crossProduct*() : (RDD[T], RDD[U]) -> RDD[(T,U)]\n",
    "- *mapValues*(f: V -> W) : RDD[(K,V)] -> RDD[(K,W)] (preserves partitioning)\n",
    "- *sort*(c: Comparator[K]) :  RDD[(K,V)] -> RDD[(K,V)]\n",
    "- *partitionBy*(p: Partitioner[K]) : RDD[(K,V)] -> RDD[(K,V)]\n",
    "\n",
    "**Actions:**\n",
    "\n",
    "- *count*() : RDD[T] -> Long\n",
    "- *collect*() : RDD[T] -> Seq[T]\n",
    "- *reduce*(f: (T,T) -> T) : RDD[T] -> T\n",
    "- *lookup*(k : K) : RDD[(K,V)] -> Seq[V] (on hash/range partitionied RDDs)\n",
    "- *save*(path: String) : Outputs RDD to a storage system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache versus Cached\n",
    "\n",
    "Count how many lines there are in the file\n",
    "\n",
    "```\n",
    "scala> spark.time(textFile.count)\n",
    "```\n",
    "\n",
    "Recount\n",
    "\n",
    "```\n",
    "scala> spark.time(textFile.count)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How to run wordcount\n",
    "\n",
    "```\n",
    "scala> val wordcount = textFile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey{(x, y) => x + y}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Save variable to HDFS file:\n",
    "\n",
    "```\n",
    "scala> wordcount.saveAsTextFile(\"output-wordcount-01\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "View the output file by calling system command from inside Scala shell\n",
    "\n",
    "```\n",
    "scala> import sys.process._\n",
    "scala> \"hdfs dfs -cat output-wordcount-01/part-00000\" #| \"head -n 20\" !\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-by-step actions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "View the top lines of `gutenberg-shakespeare.txt`:\n",
    "\n",
    "```\n",
    "scala> \"hdfs dfs -cat /repository/gutenberg-shakespeare.txt\" #| \"head -n 100\" !\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Step 1: \n",
    "\n",
    "```\n",
    "scala> val wc_step_01 = textFile.flatMap(line => line.split(\" \"))\n",
    "scala> wc_step_01\n",
    "scala> wc_step_01.take(20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Step 2:\n",
    "\n",
    "```\n",
    "scala> val wc_step_02 = wc_step_01.map(word => (word, 1))\n",
    "scala> wc_step_02.take(20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Step 3:\n",
    "\n",
    "```\n",
    "scala> val wc_step_03 = wc_step_02.reduceByKey((x, y) => x + y)\n",
    "scala> wc_step_03.saveAsTextFile(\"output-wordcount-02\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "scala> \"hdfs dfs -ls output-wordcount-01/\"!\n",
    "scala> \"hdfs dfs -ls output-wordcount-02/\"!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "- Augment the mapping process of WordCount with a function to filter out punctuations and capitalization from the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the Spark job, call `scala> :quit`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
