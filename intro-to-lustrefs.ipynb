{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Introduction to LustreFS, BeeGFS, and CephFS\n",
    "    \n",
    "  <center>  Linh B. Ngo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lustre\n",
    "\n",
    "- Started as a Carnegie Mellon research project in 1999\n",
    "- Lustre = Linux + cluster\n",
    "- Funding came from the Advanced Simulation and Computing Program (DOE/NNSA - National Nuclear Security Administration)\n",
    "- Acquired by Sun Microsystem in 2008\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lustre\n",
    "\n",
    "- Massively parallel distributed file system\n",
    "  - Thousands of clients\n",
    "  - Large capacities (55PB at LLNAL)\n",
    "  - High bandwidth (1.5TB/s at ORNL)\n",
    "  - POSIX compliance\n",
    "- Open source (GPLv2)\n",
    "- Used by many of the TOP500 supercomputers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lustre Features\n",
    "\n",
    "- File striping across disks and servers\n",
    "- Multiple metadata servers\n",
    "- Online file system checking\n",
    "- HSM (Hierarchical Storage Management) integration (thinks backup tape!)\n",
    "- User and group quotas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lustre Features\n",
    "- Pluggable Network Request Scheduler\n",
    "  - Two queues: One global queues and one per-object queue\n",
    "  - Order execution of I/O request that belong to the same data object, by offset, as close as possible to reduce disk seeks. \n",
    "- RDMA support (Remote Direct Memory Access)\n",
    "- High availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Lustre Features\n",
    "- I/O routing between networks\n",
    "- Multiple backend stroage formats (ldiskfs and zfs)\n",
    "- Storage pools\n",
    "- CPU partitions\n",
    "- Recovery features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/pfs/lustre2.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lustre Components\n",
    "- MFS: Manages filenames and directories, file stripe locations, locking, ACLs, etc.\n",
    "- MDT: Block device used by MDS to store metadata information\n",
    "- OSS: Handles I/O requests for file data\n",
    "- OST: Block device used by OSS to store file data. \n",
    "- MGS: Stores configuration information for one or more Lustre file systems\n",
    "- MGT: Block device used by MGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/pfs/lustre1.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LNET (Lustre Networking) Transport Layer \n",
    "- Provides the underlying communication infrastructure\n",
    "- Is an abstraction for underlying networking type\n",
    "  - TCP/IP\n",
    "  - Infiniband\n",
    "  - Cray high-speed interconnects\n",
    "- Allows fine-grained control of data flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/pfs/lustre3.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lustre File Striping\n",
    "- Basic properties:\n",
    "  - *stripe_count* (the number of OSTs to stripe across)\n",
    "  - *stripe_size* (how much data is written to an OST)\n",
    "- Can be customized\n",
    "- First *stripe_size* bytes are written to the first OST, second to the second OST, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/pfs/lustre4.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### I/O Flow in Lustre\n",
    "- I/O request is sent to MDS server\n",
    "- MDS server's response:\n",
    "  - Which OSTs are used\n",
    "  - What is the stripe size of the file \n",
    "  - etc\n",
    "- Client calculates which OST holds the data\n",
    "- CLient directly contacts appropriate OTS to read/write data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### I/O Recommendation\n",
    "\n",
    "- Avoid over-striping\n",
    "  - Having more stripes does not mean faster access\n",
    "  - For file sizes of O(1GB), stripe_count = 1 is recommended\n",
    "- Avoid under-striping\n",
    "  - Very large files with low counts can fill up an OST\n",
    "  - Low stripe count can cause contention if many clients are reading/writing to separate portions of the same files\n",
    "- Avoid small I/O requests\n",
    "- Know your application's I/O pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CephFS\n",
    "\n",
    "- Presented as a paper at OSDI (Operating System Design and Implementation) in 2006 \n",
    "- Funded by DOE\n",
    "- Open source (LGPL 2.1)\n",
    "- Motivation\n",
    "  - As amount of data storage increases, and amount of read/write requests increase, the amount of workload on metadata servers also increase.\n",
    "  - Increase cost of reliability\n",
    "  - Limited scalability and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Design assumptions\n",
    "- Large systems are inevitably built increamentally\n",
    "- Node failures are the norm rather than the exception\n",
    "- Quality and character of workloads are constantly shifting over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/pfs/ceph1.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key System Overview\n",
    "- Decouled Data and Metadata\n",
    "- Dynamic Distributed Metadata Management\n",
    "- Reliable Autonomic Distributed Object Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decoupled Data and Metadata\n",
    "- Metadata operations (open, rename, etc) are managed by MDS cluster\n",
    "- I/O requests managed by OSD (Object Storage Device) cluster\n",
    "- There is no allocation list to map files to specific names on OSD\n",
    "  - No lookup table (reduce workload on MDS\n",
    "  - stripe objects' name are generaed using a data distribution function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dynamic Distributed Metadata Management\n",
    "- Previous work shows that metadata operations make up as much as half of typical file system  workloads\n",
    "- Dynamic Subtree Partitioning distribute the file system directory hierarchy among MDS (scale up to hundreds)\n",
    "- Dynamic partitioning of the file system allows workload to be adapted beased on current access patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reliable Automonmic Distributed Object Storage\n",
    "- The followings are delegated to OSD\n",
    "  - Data migration\n",
    "  - Replication\n",
    "  - Failure detection\n",
    "  - Failure recovery\n",
    "- Leverage OSD's computing resources to manage themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BeeGFS\n",
    "\n",
    "- In-house effort from Fraunhofer ITWM\n",
    "  - Institute for Industrial Mathematics\n",
    "  - Software simulation of mathematical modesl\n",
    "- First beta available in 2007\n",
    "- Spinoff company: ThinkParQ\n",
    "- Open source (GPL v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goals\n",
    "- Maximum performance and scalability\n",
    "- High level of flexibility\n",
    "- Robustness and ease of use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/pfs/beegfs.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main Services\n",
    "- Management service\n",
    "- Storage service\n",
    "- Metadata service\n",
    "- Client service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Management Service\n",
    "- Lightweight\n",
    "- Monitor and adminstrative tools\n",
    "- GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Metadata Service\n",
    "- Stores information about\n",
    "  - File/directory paths (abstract)\n",
    "  - Permission, ownership\n",
    "  - Location (stripe pattern)\n",
    "- Scale-out service\n",
    "- POSIX compliance\n",
    "- Each MS instance is responsible for an exclusive fraction of the global namespace\n",
    "- Storage targets have priority based on storage availability\n",
    "- Support storage pools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Storage Service\n",
    "- Scale-out service\n",
    "- Each instance can handle multiple storage targets\n",
    "- Utilizes all memory on storage server for caching purposes\n",
    "- Stripe size and number of targets per file is managed by ME (can be defaulted or customized)\n",
    "- One chunk file per storage target per file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reliability and Fault-tolerance\n",
    "- Buddy Mirroring\n",
    "- Storage servers pair up storage targets (internally or cross servers) and replicate the targets'  contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Features\n",
    "- BeeOnDemand\n",
    "  - Quickly spin up a dynamic BeeGFS system directly on storage devices of compute nodes\n",
    "- Cloud Integration\n",
    "  - Available on AWS and Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So many PFS, what to choose?\n",
    "\n",
    "- Example: http://moo.nac.uci.edu/~hjm/fhgfs_vs_gluster.html (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Is it free?\n",
    "- Does it require high-end hardware?\n",
    "- How are the metadata servers implemented?\n",
    "- How is the support system?\n",
    "- How is the reliability of the system?\n",
    "- How is the scalability of the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- How is the storage performance?\n",
    "- How is the logging facility?\n",
    "- How is the performance with regard to different network infrastructures?\n",
    "- How is the performance for small file I/O\n",
    "- How is the performance on interactive commands (ls, du, find, grep)\n",
    "- How is the adminstrative tools?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why Lustre/Cepth/OrangeFS was not chosen\n",
    "- Lustre:\n",
    "  - Fragile to software stack and driver changes\n",
    "  - Lustre was removed from Linux kernel (https://lkml.org/lkml/2018/6/16/126)\n",
    "- Cepth:\n",
    "  - Tested in 2013\n",
    "  - Not ready for production use\n",
    "- OrangeFS (PVFS):\n",
    "  - Too few traffic and mentions on the Internet\n",
    "  - Clemson is also moving away from OrangeFS"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
