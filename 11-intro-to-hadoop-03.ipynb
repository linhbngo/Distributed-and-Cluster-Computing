{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Hadoop MapReduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First principle of optimizing Hadoop workflow: **Reduce data movement in the shuffle phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r intro-to-hadoop/output-movielens-02\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-02 \\\n",
    "    -file ./codes/avgRatingMapper04.py \\\n",
    "    -mapper avgRatingMapper04.py \\\n",
    "    -file ./codes/avgRatingReducer01.py \\\n",
    "    -reducer avgRatingReducer01.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is being passed from Map to Reduce?\n",
    "- Can reducer do the same thing as mapper, that is, to load in external data?\n",
    "- If we load external data on the reduce side, do we need to do so on the map side?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile codes/avgRatingReducer02.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "current_movie = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    movie, rating = line.split(\"\\t\", 1)\n",
    "    try:\n",
    "        rating = float(rating)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if current_movie == movie:\n",
    "        current_rating_sum += rating\n",
    "        current_rating_count += 1\n",
    "    else:\n",
    "        if current_movie:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            movieTitle = movieList[current_movie][\"title\"]\n",
    "            movieGenres = movieList[current_movie][\"genre\"]\n",
    "            print (\"%s\\t%s\\t%s\" % (movieTitle, rating_average, movieGenres))    \n",
    "        current_movie = movie\n",
    "        current_rating_sum = rating\n",
    "        current_rating_count = 1\n",
    "\n",
    "if current_movie == movie:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    movieTitle = movieList[current_movie][\"title\"]\n",
    "    movieGenres = movieList[current_movie][\"genre\"]\n",
    "    print (\"%s\\t%s\\t%s\" % (movieTitle, rating_average, movieGenres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r intro-to-hadoop/output-movielens-03\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-03 \\\n",
    "    -file ./codes/avgRatingMapper02.py \\\n",
    "    -mapper avgRatingMapper02.py \\\n",
    "    -file ./codes/avgRatingReducer02.py \\\n",
    "    -reducer avgRatingReducer02.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls intro-to-hadoop/output-movielens-02\n",
    "!hdfs dfs -ls intro-to-hadoop/output-movielens-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/output-movielens-03/part-00000 \\\n",
    "    2>/dev/null | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the number shuffle bytes in this example compare to the previous example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find genres which have the highest average ratings over the years\n",
    "\n",
    "Common optimization approaches:\n",
    "\n",
    "1. In-mapper reduction of key/value pairs\n",
    "2. Additional combiner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreMapper01.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# for nonHDFS run\n",
    "movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "#movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genreList = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genreList.split(\"|\"):\n",
    "            print (\"%s\\t%s\" % (genre, rating))\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreReducer01.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "current_genre = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, rating = line.split(\"\\t\", 1)\n",
    "\n",
    "    if current_genre == genre:\n",
    "        try:\n",
    "            current_rating_sum += float(rating)\n",
    "            current_rating_count += 1\n",
    "        except ValueError:\n",
    "            continue    \n",
    "    else:\n",
    "        if current_genre:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            print (\"%s\\t%s\" % (current_genre, rating_average))    \n",
    "        current_genre = genre\n",
    "        try:\n",
    "            current_rating_sum = float(rating)\n",
    "            current_rating_count = 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "if current_genre == genre:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    print (\"%s\\t%s\" % (current_genre, rating_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r intro-to-hadoop/output-movielens-04\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-04 \\\n",
    "    -file ./codes/avgGenreMapper01.py \\\n",
    "    -mapper avgGenreMapper01.py \\\n",
    "    -file ./codes/avgGenreReducer01.py \\\n",
    "    -reducer avgGenreReducer01.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls intro-to-hadoop/output-movielens-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/output-movielens-04/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Optimization through in-mapper reduction of Key/Value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python ./codes/avgGenreMapper01.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreMapper02.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# for nonHDFS run\n",
    "# movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "movieFile = \"./movies.csv\"\n",
    "\n",
    "movieList = {}\n",
    "genreList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genres = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genres.split(\"|\"):\n",
    "            if genre in genreList:\n",
    "                genreList[genre][\"total_rating\"] += rating\n",
    "                genreList[genre][\"total_count\"] += 1\n",
    "            else:\n",
    "                genreList[genre] = {}\n",
    "                genreList[genre][\"total_rating\"] = rating\n",
    "                genreList[genre][\"total_count\"] = 1\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "for genre in genreList:\n",
    "    print (\"%s\\t%s\" % (genre, json.dumps(genreList[genre])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python ./codes/avgGenreMapper02.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreReducer02.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "current_genre = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "\n",
    "    if current_genre == genre:\n",
    "        try:\n",
    "            current_rating_sum += ratingInfo[\"total_rating\"]\n",
    "            current_rating_count += ratingInfo[\"total_count\"]\n",
    "        except ValueError:\n",
    "            continue    \n",
    "    else:\n",
    "        if current_genre:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            print (\"%s\\t%s\" % (current_genre, rating_average))    \n",
    "        current_genre = genre\n",
    "        try:\n",
    "            current_rating_sum = ratingInfo[\"total_rating\"]\n",
    "            current_rating_count = ratingInfo[\"total_count\"]\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "if current_genre == genre:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    print (\"%s\\t%s\" % (current_genre, rating_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python ./codes/avgGenreMapper02.py \\\n",
    "    | sort \\\n",
    "    | python ./codes/avgGenreReducer02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make sure that the path to movies.csv is correct inside avgGenreMapper02.py\n",
    "!hdfs dfs -rm -R intro-to-hadoop/output-movielens-05\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-05 \\\n",
    "    -file ./codes/avgGenreMapper02.py \\\n",
    "    -mapper avgGenreMapper02.py \\\n",
    "    -file ./codes/avgGenreReducer02.py \\\n",
    "    -reducer avgGenreReducer02.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/output-movielens-05/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/output-movielens-04/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How different are the number of shuffle bytes between the two jobs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Optimization through combiner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 items\r\n",
      "-rw-r--r--   2 lngo   hdfs-user       1034 2016-11-18 08:04 /repository/.pysparkrc\r\n",
      "drwxr-xr-x   - lngo   hdfs-user          0 2017-09-14 09:23 /repository/airlines\r\n",
      "-rw-r--r--   2 lngo   hdfs-user 2383967007 2016-11-29 21:31 /repository/bigdata-workshop.tgz\r\n",
      "drwxr-xr-x   - denton hdfs-user          0 2017-10-11 09:15 /repository/chicago_data\r\n",
      "-rw-r--r--   2 lngo   hdfs-user    5590193 2016-03-22 14:09 /repository/complete-shakespeare.txt\r\n",
      "drwxr-xr-x   - denton hdfs-user          0 2016-11-02 08:16 /repository/cypress-pyspark-kernel\r\n",
      "drwxr-xr-x   - lngo   hdfs-user          0 2016-02-03 10:17 /repository/gtrace\r\n",
      "drwxr-xr-x   - lngo   hdfs-user          0 2017-05-23 08:40 /repository/halvade\r\n",
      "-rw-r--r--   2 lngo   hdfs-user 2580196770 2017-03-16 06:02 /repository/intro-to-hadoop.tgz\r\n",
      "-rw-r--r--   2 denton hdfs-user      34590 2016-12-01 09:31 /repository/intro-to-pyspark.ipynb\r\n",
      "-rw-r--r--   2 lngo   hdfs-user 2775356893 2017-04-04 14:55 /repository/intro-to-spark-palmetto.tgz\r\n",
      "-rw-r--r--   2 lngo   hdfs-user 2502932465 2016-11-01 15:47 /repository/intro-to-spark.tgz\r\n",
      "-rw-r--r--   2 lngo   hdfs-user  294155091 2017-03-23 09:14 /repository/intro-to-sparkr.tgz\r\n",
      "drwxr-xr-x   - lngo   hdfs-user          0 2017-03-15 09:49 /repository/movielens\r\n",
      "-rw-r--r--   2 lngo   hdfs-user  620204630 2016-11-30 11:16 /repository/ratings.csv\r\n",
      "drwxr-xr-x   - lngo   hdfs-user          0 2016-02-24 18:58 /repository/reddit\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/10/11 12:22:53 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./codes/wordcountMapper.py, ./codes/wordcountReducer.py] [/usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.0.3-8.jar] /hadoop_java_io_tmpdir/streamjob2064724442186879809.jar tmpDir=null\n",
      "17/10/11 12:22:55 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/11 12:22:55 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/11 12:22:55 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14512 for lngo on ha-hdfs:dsci\n",
      "17/10/11 12:22:55 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14512 for lngo)\n",
      "17/10/11 12:22:56 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "17/10/11 12:22:56 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 8787857212dae53ffae3b3113abc894e6743b4ab]\n",
      "17/10/11 12:22:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/10/11 12:22:56 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/10/11 12:22:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1505269880969_0213\n",
      "17/10/11 12:22:56 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14512 for lngo)\n",
      "17/10/11 12:22:57 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "17/10/11 12:22:57 INFO impl.YarnClientImpl: Submitted application application_1505269880969_0213\n",
      "17/10/11 12:22:57 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1505269880969_0213/\n",
      "17/10/11 12:22:57 INFO mapreduce.Job: Running job: job_1505269880969_0213\n",
      "17/10/11 12:23:08 INFO mapreduce.Job: Job job_1505269880969_0213 running in uber mode : false\n",
      "17/10/11 12:23:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/10/11 12:23:15 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/10/11 12:23:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/10/11 12:23:26 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/10/11 12:23:26 INFO mapreduce.Job: Job job_1505269880969_0213 completed successfully\n",
      "17/10/11 12:23:26 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8575076\n",
      "\t\tFILE: Number of bytes written=17639352\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5678879\n",
      "\t\tHDFS: Number of bytes written=721220\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=42006\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23682\n",
      "\t\tTotal time spent by all map tasks (ms)=14002\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7894\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14002\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7894\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=180513784\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=101769448\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=124796\n",
      "\t\tMap output records=904087\n",
      "\t\tMap output bytes=6766896\n",
      "\t\tMap output materialized bytes=8575082\n",
      "\t\tInput split bytes=198\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=67799\n",
      "\t\tReduce shuffle bytes=8575082\n",
      "\t\tReduce input records=904087\n",
      "\t\tReduce output records=67799\n",
      "\t\tSpilled Records=1808174\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=228\n",
      "\t\tCPU time spent (ms)=10080\n",
      "\t\tPhysical memory (bytes) snapshot=4022095872\n",
      "\t\tVirtual memory (bytes) snapshot=39908544512\n",
      "\t\tTotal committed heap usage (bytes)=4083154944\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5678681\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=721220\n",
      "17/10/11 12:23:26 INFO streaming.StreamJob: Output directory: intro-to-hadoop/output-wordcount-01\n"
     ]
    }
   ],
   "source": [
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/complete-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount-01 \\\n",
    "    -file ./codes/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file ./codes/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/10/11 12:23:28 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./codes/wordcountMapper.py, ./codes/wordcountReducer.py] [/usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.0.3-8.jar] /hadoop_java_io_tmpdir/streamjob3330378274220223963.jar tmpDir=null\n",
      "17/10/11 12:23:29 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/11 12:23:30 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/11 12:23:30 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14515 for lngo on ha-hdfs:dsci\n",
      "17/10/11 12:23:30 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14515 for lngo)\n",
      "17/10/11 12:23:30 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "17/10/11 12:23:30 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 8787857212dae53ffae3b3113abc894e6743b4ab]\n",
      "17/10/11 12:23:30 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/10/11 12:23:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/10/11 12:23:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1505269880969_0216\n",
      "17/10/11 12:23:31 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14515 for lngo)\n",
      "17/10/11 12:23:31 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "17/10/11 12:23:31 INFO impl.YarnClientImpl: Submitted application application_1505269880969_0216\n",
      "17/10/11 12:23:31 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1505269880969_0216/\n",
      "17/10/11 12:23:31 INFO mapreduce.Job: Running job: job_1505269880969_0216\n",
      "17/10/11 12:23:38 INFO mapreduce.Job: Job job_1505269880969_0216 running in uber mode : false\n",
      "17/10/11 12:23:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/10/11 12:23:46 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/10/11 12:23:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/10/11 12:23:52 INFO mapreduce.Job: Job job_1505269880969_0216 completed successfully\n",
      "17/10/11 12:23:52 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1105615\n",
      "\t\tFILE: Number of bytes written=2701408\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5678879\n",
      "\t\tHDFS: Number of bytes written=717253\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=32610\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11886\n",
      "\t\tTotal time spent by all map tasks (ms)=10870\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3962\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10870\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3962\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=140136040\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=51078104\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=124796\n",
      "\t\tMap output records=904087\n",
      "\t\tMap output bytes=6766896\n",
      "\t\tMap output materialized bytes=1105621\n",
      "\t\tInput split bytes=198\n",
      "\t\tCombine input records=904087\n",
      "\t\tCombine output records=89121\n",
      "\t\tReduce input groups=67799\n",
      "\t\tReduce shuffle bytes=1105621\n",
      "\t\tReduce input records=89121\n",
      "\t\tReduce output records=67799\n",
      "\t\tSpilled Records=178242\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=160\n",
      "\t\tCPU time spent (ms)=9020\n",
      "\t\tPhysical memory (bytes) snapshot=5466710016\n",
      "\t\tVirtual memory (bytes) snapshot=39907373056\n",
      "\t\tTotal committed heap usage (bytes)=5898764288\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5678681\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=717253\n",
      "17/10/11 12:23:52 INFO streaming.StreamJob: Output directory: intro-to-hadoop/output-wordcount-02\n"
     ]
    }
   ],
   "source": [
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/complete-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount-02 \\\n",
    "    -file ./codes/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file ./codes/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py \\\n",
    "    -combiner wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/avgGenreCombiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/avgGenreCombiner.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "genreList = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "\n",
    "    if genre in genreList:\n",
    "        genreList[genre][\"total_rating\"] += ratingInfo[\"total_rating\"]\n",
    "        genreList[genre][\"total_count\"] += ratingInfo[\"total_count\"]\n",
    "    else:\n",
    "        genreList[genre] = {}\n",
    "        genreList[genre][\"total_rating\"] = ratingInfo[\"total_rating\"]\n",
    "        genreList[genre][\"total_count\"] = 1\n",
    "\n",
    "for genre in genreList:\n",
    "    print (\"%s\\t%s\" % (genre, json.dumps(genreList[genre])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/10/11 12:28:37 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/lngo/intro-to-hadoop/output-movielens-06' to trash at: hdfs://dsci/user/lngo/.Trash/Current/user/lngo/intro-to-hadoop/output-movielens-06\n",
      "17/10/11 12:28:39 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./codes/avgGenreMapper02.py, ./codes/avgGenreReducer02.py, ./codes/avgGenreCombiner.py, ./movielens/movies.csv] [/usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.0.3-8.jar] /hadoop_java_io_tmpdir/streamjob706582146786084890.jar tmpDir=null\n",
      "17/10/11 12:28:40 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/11 12:28:40 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/10/11 12:28:41 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14539 for lngo on ha-hdfs:dsci\n",
      "17/10/11 12:28:41 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14539 for lngo)\n",
      "17/10/11 12:28:41 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "17/10/11 12:28:41 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 8787857212dae53ffae3b3113abc894e6743b4ab]\n",
      "17/10/11 12:28:41 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/10/11 12:28:41 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "17/10/11 12:28:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1505269880969_0238\n",
      "17/10/11 12:28:41 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14539 for lngo)\n",
      "17/10/11 12:28:42 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "17/10/11 12:28:42 INFO impl.YarnClientImpl: Submitted application application_1505269880969_0238\n",
      "17/10/11 12:28:42 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1505269880969_0238/\n",
      "17/10/11 12:28:42 INFO mapreduce.Job: Running job: job_1505269880969_0238\n",
      "17/10/11 12:28:48 INFO mapreduce.Job: Job job_1505269880969_0238 running in uber mode : false\n",
      "17/10/11 12:28:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/10/11 12:29:01 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "17/10/11 12:29:02 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "17/10/11 12:29:04 INFO mapreduce.Job:  map 39% reduce 0%\n",
      "17/10/11 12:29:05 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "17/10/11 12:29:07 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "17/10/11 12:29:08 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "17/10/11 12:29:09 INFO mapreduce.Job:  map 91% reduce 0%\n",
      "17/10/11 12:29:10 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/10/11 12:29:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/10/11 12:29:13 INFO mapreduce.Job: Job job_1505269880969_0238 completed successfully\n",
      "17/10/11 12:29:13 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5603\n",
      "\t\tFILE: Number of bytes written=995397\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=663945432\n",
      "\t\tHDFS: Number of bytes written=1653\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=258180\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11277\n",
      "\t\tTotal time spent by all map tasks (ms)=86060\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3759\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=86060\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3759\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1109485520\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=48461028\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24404097\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=5899\n",
      "\t\tMap output materialized bytes=5627\n",
      "\t\tInput split bytes=480\n",
      "\t\tCombine input records=100\n",
      "\t\tCombine output records=100\n",
      "\t\tReduce input groups=92\n",
      "\t\tReduce shuffle bytes=5627\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=92\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=3269\n",
      "\t\tCPU time spent (ms)=144870\n",
      "\t\tPhysical memory (bytes) snapshot=14061768704\n",
      "\t\tVirtual memory (bytes) snapshot=79764459520\n",
      "\t\tTotal committed heap usage (bytes)=15988686848\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=663944952\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1653\n",
      "17/10/11 12:29:13 INFO streaming.StreamJob: Output directory: intro-to-hadoop/output-movielens-06\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r intro-to-hadoop/output-movielens-06\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-06 \\\n",
    "    -file ./codes/avgGenreMapper02.py \\\n",
    "    -mapper avgGenreMapper02.py \\\n",
    "    -file ./codes/avgGenreReducer02.py \\\n",
    "    -reducer avgGenreReducer02.py \\\n",
    "    -file ./codes/avgGenreCombiner.py \\\n",
    "    -combiner avgGenreCombiner.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How different are the number of shuffle bytes between the two jobs?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (Anaconda)",
   "language": "python",
   "name": "anaconda_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
