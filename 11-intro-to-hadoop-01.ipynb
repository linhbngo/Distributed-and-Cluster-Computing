{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Hadoop MapReduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Jupyter notebook supports execution of Linux command inside the notebook cells. This is done by adding the **!** to the beginning of the command line. It should be noted that each command begins with a **!** will create a new bash shell and close this cell once the execution is done:\n",
    "- Full path is required\n",
    "- Temporary results and environmental variables will be lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data into Hadoop\n",
    "\n",
    "Upload the **text** directory into the newly created **intro-to-hadoop** directory. \n",
    "\n",
    "```\n",
    "$ wget https://raw.githubusercontent.com/linhbngo/Distributed-and-Cluster-Computing/master/text/gutenberg-shakespeare.txt\n",
    "$ hdfs dfs -put gutenberg-shakespeare.txt intro-to-hadoop/\n",
    "$ hdfs dfs -ls intro-to-hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a sample MapReduce program\n",
    "\n",
    "```\n",
    "$ yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples-3.1.1.3.0.1.0-187.jar wordcount intro-to-hadoop/gutenberg-shakespeare.txt output-wordcount\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "\n",
    "```\n",
    "18/11/01 17:03:38 INFO client.RMProxy: Connecting to ResourceManager at clnode188.clemson.cloudlab.us/130.127.133.197:8050\n",
    "18/11/01 17:03:39 INFO client.AHSProxy: Connecting to Application History server at clnode195.clemson.cloudlab.us/130.127.133.204:10200\n",
    "18/11/01 17:03:39 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/lngo/.staging/job_1541104508981_0008\n",
    "18/11/01 17:03:39 INFO input.FileInputFormat: Total input files to process : 1\n",
    "18/11/01 17:03:39 INFO mapreduce.JobSubmitter: number of splits:1\n",
    "18/11/01 17:03:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1541104508981_0008\n",
    "18/11/01 17:03:39 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
    "18/11/01 17:03:39 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.0.1.0-187/0/resource-types.xml\n",
    "18/11/01 17:03:40 INFO impl.YarnClientImpl: Submitted application application_1541104508981_0008\n",
    "18/11/01 17:03:40 INFO mapreduce.Job: The url to track the job: http://clnode188.clemson.cloudlab.us:8088/proxy/application_1541104508981_0008/\n",
    "18/11/01 17:03:40 INFO mapreduce.Job: Running job: job_1541104508981_0008\n",
    "18/11/01 17:03:44 INFO mapreduce.Job: Job job_1541104508981_0008 running in uber mode : false\n",
    "18/11/01 17:03:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "18/11/01 17:03:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "18/11/01 17:03:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "18/11/01 17:03:54 INFO mapreduce.Job: Job job_1541104508981_0008 completed successfully\n",
    "18/11/01 17:03:54 INFO mapreduce.Job: Counters: 53\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=973082\n",
    "                FILE: Number of bytes written=2409015\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=5447902\n",
    "                HDFS: Number of bytes written=713504\n",
    "                HDFS: Number of read operations=8\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=2\n",
    "        Job Counters\n",
    "                Launched map tasks=1\n",
    "                Launched reduce tasks=1\n",
    "                Data-local map tasks=1\n",
    "                Total time spent by all maps in occupied slots (ms)=282360\n",
    "                Total time spent by all reduces in occupied slots (ms)=266760\n",
    "                Total time spent by all map tasks (ms)=3620\n",
    "                Total time spent by all reduce tasks (ms)=1710\n",
    "                Total vcore-milliseconds taken by all map tasks=3620\n",
    "                Total vcore-milliseconds taken by all reduce tasks=1710\n",
    "                Total megabyte-milliseconds taken by all map tasks=289136640\n",
    "                Total megabyte-milliseconds taken by all reduce tasks=273162240\n",
    "        Map-Reduce Framework\n",
    "                Map input records=124213\n",
    "                Map output records=899681\n",
    "                Map output bytes=8529629\n",
    "                Map output materialized bytes=973082\n",
    "                Input split bytes=158\n",
    "                Combine input records=899681\n",
    "                Combine output records=67109\n",
    "                Reduce input groups=67109\n",
    "                Reduce shuffle bytes=973082\n",
    "                Reduce input records=67109\n",
    "                Reduce output records=67109\n",
    "                Spilled Records=134218\n",
    "                Shuffled Maps =1\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=1\n",
    "                GC time elapsed (ms)=705\n",
    "                CPU time spent (ms)=28770\n",
    "                Physical memory (bytes) snapshot=3245010944\n",
    "                Virtual memory (bytes) snapshot=210530725888\n",
    "                Total committed heap usage (bytes)=3762814976\n",
    "                Peak Map Physical memory (bytes)=2774241280\n",
    "                Peak Map Virtual memory (bytes)=70509170688\n",
    "                Peak Reduce Physical memory (bytes)=470769664\n",
    "                Peak Reduce Virtual memory (bytes)=140021555200\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters\n",
    "                Bytes Read=5447744\n",
    "        File Output Format Counters\n",
    "                Bytes Written=713504\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Output\n",
    "\n",
    "```\n",
    "$ hdfs dfs -cat output-wordcount/part-r-00000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Hello World of Hadoop: Word Count\n",
    "\n",
    "[Example Source Code](https://github.com/apache/hadoop/blob/branch-2.7.3/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
