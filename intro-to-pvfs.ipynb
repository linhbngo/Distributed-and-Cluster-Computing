{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Introduction to PVFS, a Parallel File System for Linux\n",
    "    \n",
    "  <center>  Linh B. Ngo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why not NFS?\n",
    "\n",
    "The issue is with NFS's centralized storage model:\n",
    "- Scalability: The increasing number of computing nodes will overwhelm the performance capacity of the NFS server.\n",
    "- Availability: If the NFS server goes down, all processing nodes will have to wait. This issue becomes more severe as the number of compute nodes increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (One of) The Solution(s): Parallel Virtual File System\n",
    "\n",
    "- Research and development began in 1993\n",
    "- Funded through NASA's grant to study I/O patterns of parallel programs\n",
    "  - This is also about time when NASA funded the seminal work of Dr. Thomas Sterling to study how to develop an inexpensive *supercomputer*.\n",
    "- First version released in 1998/1999\n",
    "- Second version released in 2005\n",
    "- Omnibond, a Clemson spin-off company was formed to sell supports and services (all versions of PVFS are open source)\n",
    "- Name changed to OrangeFS since 2011\n",
    "  - OrangeFS supported by Omnibond/Clemson team (main)\n",
    "  - Blue branch supported by Argonne National Lab for an IMB Blue-Gene supercomputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Timeliness\n",
    "\n",
    "- Thomas Sterling's seminal paper on the design of a Beowulf cluster\n",
    "  - Large cluster consisting of inexpensive COTS (commodity off-the-shelf) machines\n",
    "- Open source Linux operating system\n",
    "- Standardization of MPI as a tool for large-scale programming\n",
    "- An open source parallel file system was the only thing that was missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goals\n",
    "- High performance\n",
    "  - High concurrent read/write bandwidth from multiple proceses or threads to **a common file**\n",
    "  - Mutiple APIs: native PVFS, UNIX/POSIX, and MPI-IO\n",
    "  - Common UNIX shell commands must work\n",
    "  - Application developed for UNIX must be able to access PVFS files without recompiling\n",
    "- Robust and scalable\n",
    "- Easy to install and use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Parallel Virtual File System (PVFS)\n",
    "- Parallel: Data are physically stored on multiple independent machines with separate network connections\n",
    "- Virtual: There exists a set of *virtual* interfaces (maintained through daemon processes) between the physical storage and a user-space abstraction. \n",
    "- File System: Through this abstraction, users can store and retrieve data using common file access methods applicable to traditional file systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parallel Virtual File System (PVFS)\n",
    "- Unlike the centrallized model of NFS, PVFS has N servers making portions of a file available to the tasks of a parallel application running on multiple processing nodes over a network\n",
    "- The aggregate bandwidth exceeds that of a single machine\n",
    "- This works in a manner similar to how a RAID0 disk array works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### System Architecture\n",
    "- I/O Nodes: stores the actual files, connected to disks holding the physical bytes of the data. Each file is striped across the disks on the I/O nodes\n",
    "- Management Node: handles metadata operations\n",
    "- Compute Nodes: where parallal applications are executed. These applications will interact with PVFS via APIs (native PVFS, MPI-IO, or UNIX/POSIX I/O)\n",
    "- *A physical node may perform more than one role*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/intro-to-pvfs/arch1.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PVFS I/O Nodes\n",
    "- By spreading data acrosss multiple I/O Nodes, applications have multiple paths to data through the network and through multiple disks on which data is stored.\n",
    "- Significantly reduces potential bottles nects in the I/O path.\n",
    "- Significantly increase total potential bandwidth for multiple clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PVFS Software Components\n",
    "- There are four major components:\n",
    "  - Metadata server (mgr)\n",
    "  - I/O server (iod)\n",
    "  - API (native PVFS, MPI-IO, POSIX)\n",
    "  - PVFS Linux kernel support\n",
    "- The first two components are deamons which run on nodes in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Metadata Server\n",
    "- Manage metadata for PVFS files\n",
    "- Version 1: A single manager daemon is responsible for storaging and accessing of all metadata\n",
    "- Current: Manager servers are distributed across I/O nodes\n",
    "- Metadata:\n",
    "  - Permissions\n",
    "  - Ownership\n",
    "  - **Physical distribution on I/O nodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### File distribution\n",
    "- Data in a file is striped across a set of I/O nodes in order to facilitate parallel access\n",
    "- The specific of this striping process (distribution process) can be described with three metadata parameters\n",
    "  - Base I/O node number\n",
    "  - Number of I/O nodes\n",
    "  - Stripe size\n",
    "- These parameters, together with an ordering of the I/O nodes for the file system, allow the file distribution to be completely specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/intro-to-pvfs/stripes.gif\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The I/O Server (IOD)\n",
    "\n",
    "- Handles storing and retrieving of data file stored on local disks connected to the node. \n",
    "- When application processs access a PVFS file, the PVFS manager informs them of the locations of the I/O daemons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/intro-to-pvfs/getmetadata.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The processes then establish connections with the I/O daemons directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/intro-to-pvfs/getdata.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PVFS native API (libpvfs)\n",
    "- Provides users with a mean to transparently access PVFS servers\n",
    "- Handles the scatter/gather operations necessary to move data between user buffers and PVFS servers\n",
    "- Handles communications related to metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PVFS Linux Kernel Support\n",
    "- Provides the functionality necessary to mount PVFS file systems on Linux nodes\n",
    "- Enables existing programs to access PVFS without any modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/intro-to-pvfs/kernel.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusion\n",
    "- Pros:\n",
    "  - Better performance than NFS\n",
    "  - Better scalability (both performance and storage size)\n",
    "  - Ready and easy to use\n",
    "  - Optimize for large amount of reading/writing on small amount of files\n",
    "- Cons:\n",
    "  - Multiple points of failure\n",
    "  - Not good for *interactive* work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hands-on\n",
    "- Open a Linux terminal\n",
    "- SSH to 130.127.132.226 (student/goram)\n",
    "    - `ssh student@130.127.132.226`\n",
    "- Run the following commands\n",
    "```\n",
    "$ cd /scratch\n",
    "$ ls -l /scratch/ml-20m\n",
    "$ pvfs2-stat ml-20m/ratings.csv\n",
    "$ pvfs2-viewdist -f ml-20m/ratings.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> References\n",
    "Ligon III, Walter B., and Robert B. Ross. \"An overview of the parallel virtual file system.\" In Proceedings of the 1999 Extreme Linux Workshop. 1999."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
