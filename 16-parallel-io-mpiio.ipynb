{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>MPIIO - Distributed File Systems</center>\n",
    "### <center> Linh B. Ngo </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Common Ways of Doing I/O in Parallel Programs </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**All processes send data to rank 0, and 0 writes it to the file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/mpiio1.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Pros:\n",
    "    - parallel machine may support I/O from only one process (e.g., no common file system)\n",
    "    - some I/O libraries (e.g. HDF-4, NetCDF) not parallel\n",
    "    - resulting single file is handy for ftp, mv\n",
    "    - big blocks improve performance\n",
    "    - short distance from original, serial code\n",
    "- Cons:\n",
    "    - lack of parallelism limits scalability, performance (single node bottleneck)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Each process writes to a separate file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/mpiio2.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Pros: \n",
    "    - parallelism, high performance\n",
    "- Cons:  \n",
    "    - lots of small files to manage\n",
    "    - difficult to read back data from different number of processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center>MPI-IO Approach</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** What is Parallel I/O? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multiple processes of a parallel program accessing data (reading or writing) from a common file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/mpiio3.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Why Parallel I/O? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Non-parallel I/O is simple but\n",
    "    - Poor performance (single process writes to one file) or\n",
    "    - Awkward and not interoperable with other tools (each process writes a separate file)\n",
    "- Parallel I/O\n",
    "    - Provides high performance\n",
    "    - Can provide a single file that can be used with other tools (such as visualization programs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Why is MPI a good setting for Parallel I/O? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Writing is like sending a message and reading is like receiving\n",
    "- Any parallel I/O system will need a mechanism to\n",
    "    - define collective operations (MPI communicators)\n",
    "    - define noncontiguous data layout in memory and file (MPI datatypes)\n",
    "    - test completion of nonblocking operations (MPI request objects)\n",
    "- i.e., lots of MPI-like machinery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Four stages\n",
    "    * Open File\n",
    "    * Set File View (optional)\n",
    "    * Read or Write Data\n",
    "    * Close File\n",
    "- All the complexity is hidden in setting the file view\n",
    "- Write is probably more important in practice than read\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Opening a File  **\n",
    "\n",
    "~~~\n",
    "int MPI_File_open(MPI_Comm comm, const char *filename, int amode, MPI_Info info, MPI_File *fh)\n",
    "~~~\n",
    "\n",
    "- amode: File access mode (integer)\n",
    "- info: Info object (handle)\n",
    "- fh: New file handle (handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- MPI_File_open opens the file identified by the filename filename on all processes in the comm communicator group. \n",
    "- MPI_File_open is a collective routine; all processes must provide the same value for amode, and all processes must provide filenames that reference the same file and which are textually identical. A process can open a file independently of other processes by using the MPI_COMM_SELF communicator. \n",
    "- The file handle returned, fh, can be subsequently used to access the file until the file is closed using MPI_File_close. Before calling MPI_Finalize, the user is required to close (via MPI_File_close) all files that were opened with MPI_File_open. \n",
    "- Initially, all processes view the file as a linear byte stream; that is, the etype and filetype are both MPI_BYTE. The file view can be changed via the MPI_File_set_view routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- MPI_MODE_APPEND \n",
    "- MPI_MODE_CREATE -- Create the file if it does not exist. \n",
    "- MPI_MODE_DELETE_ON_CLOSE \n",
    "- MPI_MODE_EXCL -- Error creating a file that already exists. \n",
    "- MPI_MODE_RDONLY -- Read only. \n",
    "- MPI_MODE_RDWR -- Reading and writing. \n",
    "- MPI_MODE_SEQUENTIAL \n",
    "- MPI_MODE_WRONLY -- Write only. \n",
    "- MPI_MODE_UNIQUE_OPEN\n",
    "\n",
    "The modes MPI_MODE_RDONLY, MPI_MODE_RDWR, MPI_MODE_WRONLY, and MPI_MODE_CREATE have identical semantics to their POSIX counterparts. It is erroneous to specify MPI_MODE_CREATE in conjunction with MPI_MODE_RDONLY. Errors related to the access mode are raised in the class MPI_ERR_AMODE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Set File View  **\n",
    "~~~\n",
    "int MPI_File_set_view(MPI_File fh, MPI_Offset disp, MPI_Datatype etype, MPI_Datatype filetype, const char *datarep, MPI_Info info)\n",
    "~~~\n",
    "\n",
    "- disp: Displacement (integer).\n",
    "- etype: Elementary data type (handle).\n",
    "- filetype: File type (handle). \n",
    "- datarep: Data representation (string).\n",
    "- info: Info object (handle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/fileview66pct.gif\" width=\"700\"/> \n",
    "</center>\n",
    "*https://cvw.cac.cornell.edu/parallelio/fileviews*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Reading a File **\n",
    "~~~\n",
    "int MPI_File_read(MPI_File fh, void *buf,  int count, MPI_Datatype datatype, MPI_Status *status)\n",
    "~~~\n",
    "\n",
    "- fh File handle (handle).\n",
    "- count Number of elements in buffer (integer).\n",
    "- datatype Data type of each buffer element (handle).\n",
    "- buf Initial address of buffer (integer).\n",
    "- status Status object (status).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Writing to a File **\n",
    "\n",
    "~~~\n",
    "int MPI_File_write(MPI_File fh, void *buf, int count, MPI_Datatype datatype, MPI_Status *status)\n",
    "~~~\n",
    "\n",
    "- fh: file handle (handle)\n",
    "- buf: initial address of buffer (choice)\n",
    "- count: number of elements in buffer (nonnegative integer)\n",
    "- datatype: datatype of each buffer element (handle)\n",
    "- status: status object (Status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Seeking inside a File  **\n",
    "~~~\n",
    "int MPI_File_seek(MPI_File fh, MPI_Offset offset, int whence)\n",
    "~~~\n",
    "\n",
    "- fh File handle (handle).\n",
    "- offset File offset (integer).\n",
    "- whence Update mode (integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MPI_File_seek updates the individual file pointer according to whence, which could have the following possible values: \n",
    "\n",
    "- MPI_SEEK_SET - The pointer is set to offset. \n",
    "- MPI_SEEK_CUR - The pointer is set to the current pointer position plus offset. \n",
    "- MPI_SEEK_END - The pointer is set to the end of the file plus offset.\n",
    "\n",
    "The offset can be negative, which allows seeking backwards. It is erroneous to seek to a negative position in the file. The end of the file is defined to be the location of the next elementary data item immediately after the last accessed data item, even if that location is a hole.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Closing a File  **\n",
    "~~~\n",
    "MPI_File_close(MPI_File *fh)\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/openmpi/mpiio_seqwrite.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/openmpi/mpiio_seqwrite.c\n",
    "\n",
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "#define array_size 4\n",
    "\n",
    "static char filename[] = \"output.dat\";\n",
    "\n",
    "main(int argc, char **argv)\n",
    "{\n",
    "  int rank, size;\n",
    "  MPI_File outfile;\n",
    "  MPI_Status status;\n",
    "  MPI_Datatype arraytype;\n",
    "  int nbytes, myarray[array_size], mode, i;\n",
    "  double start, stop, resolution;\n",
    "    \n",
    "  /* For testing purposes */\n",
    "  FILE *fptr;\n",
    "  int oneNum;\n",
    "\n",
    "  /* initialize MPI */\n",
    "  MPI_Init( &argc, &argv );\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    \n",
    "  MPI_Type_contiguous(array_size, MPI_INT, &arraytype);\n",
    "  MPI_Type_commit(&arraytype);\n",
    "\n",
    "  /* initialize array */\n",
    "  for (i=0; i < array_size; i++) {\n",
    "    myarray[i] = rank;\n",
    "  }\n",
    "\n",
    "  /* open file */\n",
    "  mode = MPI_MODE_CREATE | MPI_MODE_WRONLY;\n",
    "  MPI_File_open(MPI_COMM_WORLD, filename, mode, MPI_INFO_NULL, &outfile);\n",
    "\n",
    "  /* set file view */\n",
    "  MPI_File_set_view(outfile, rank * array_size * sizeof(MPI_INT), MPI_INT, arraytype, \"native\", MPI_INFO_NULL);\n",
    "\n",
    "  /*  write buffer to file*/\n",
    "  MPI_File_write(outfile, &myarray[0], array_size, MPI_INT, &status);    \n",
    "\n",
    "  /* print out number of bytes written */\n",
    "  MPI_Get_elements(&status, MPI_CHAR, &nbytes);\n",
    "  printf(\"TASK %d wrote %d bytes\\n\", rank, nbytes);\n",
    "\n",
    "  /* close file */\n",
    "  MPI_File_close( &outfile );\n",
    "\n",
    "  MPI_Barrier(MPI_COMM_WORLD);\n",
    "\n",
    "  \n",
    "    \n",
    "  /* finalize MPI */\n",
    "  MPI_Finalize();\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 0 wrote 16 bytes\n",
      "TASK 1 wrote 16 bytes\n",
      "TASK 2 wrote 16 bytes\n",
      "TASK 3 wrote 16 bytes\n",
      "00000000  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n",
      "*\n",
      "00000020  01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00  |................|\n",
      "00000030  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n",
      "00000040  02 00 00 00 02 00 00 00  02 00 00 00 02 00 00 00  |................|\n",
      "00000050  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n",
      "00000060  03 00 00 00 03 00 00 00  03 00 00 00 03 00 00 00  |................|\n",
      "00000070\n"
     ]
    }
   ],
   "source": [
    "!mpicc codes/openmpi/mpiio_seqwrite.c -o ~/mpiio_seqwrite\n",
    "!rm output.dat\n",
    "!mpirun -np 4 --map-by core:OVERSUBSCRIBE ~/mpiio_seqwrite\n",
    "!hexdump -C output.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/openmpi/mpiio_seqread.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/openmpi/mpiio_seqread.c\n",
    "\n",
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "#define array_size 4\n",
    "\n",
    "static char filename[] = \"output.dat\"; \n",
    "\n",
    "main(int argc, char **argv)\n",
    "{\n",
    "    int rank, size;\n",
    "    MPI_File infile;\n",
    "    MPI_Status status;\n",
    "    int nbytes, myarray[array_size], mode, i;\n",
    "    double start, stop;\n",
    "\n",
    "    /* initialize MPI */\n",
    "    MPI_Init( &argc, &argv );\n",
    "    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n",
    "    MPI_Comm_size( MPI_COMM_WORLD, &size );\n",
    "\n",
    "    /* open file */\n",
    "    mode = MPI_MODE_RDONLY;\n",
    "\n",
    "    MPI_File_open( MPI_COMM_WORLD, filename, mode, MPI_INFO_NULL, &infile );\n",
    "\n",
    "    /* set file view */\n",
    "    MPI_File_set_view( infile, rank*array_size*sizeof(MPI_INT), MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL );\n",
    "\n",
    "    /*  read file */\n",
    "    MPI_File_read( infile, &myarray[0], array_size, MPI_INT, &status );\n",
    "\n",
    "\n",
    "    /* close file */\n",
    "    MPI_File_close( &infile );\n",
    "\n",
    "    /* print out results */\n",
    "    for (i=0; i < array_size; i++) \n",
    "      printf(\"%2d%c\", myarray[i], i%4==3 ? '\\n' : ' ');\n",
    "\n",
    "    /* finalize MPI */\n",
    "    MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  0  1  0\r\n",
      " 0  0  0  0\r\n",
      " 0  0  1  0\r\n",
      " 0  0  0  0\r\n"
     ]
    }
   ],
   "source": [
    "!mpicc codes/openmpi/mpiio_seqread.c -o ~/mpiio_seqread\n",
    "!mpirun -np 4 --map-by core:OVERSUBSCRIBE ~/mpiio_seqread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/openmpi/mpiio_cirwrite.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/openmpi/mpiio_cirwrite.c\n",
    "\n",
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "#define array_size 4\n",
    "\n",
    "static char filename[] = \"output.dat\";\n",
    "\n",
    "main(int argc, char **argv)\n",
    "{\n",
    "  int rank, size;\n",
    "  MPI_File outfile;\n",
    "  MPI_Status status;\n",
    "  MPI_Datatype arraytype;\n",
    "  int nbytes, myarray[array_size], mode, i;\n",
    "  double start, stop, resolution;\n",
    "\n",
    "  /* initialize MPI */\n",
    "  MPI_Init( &argc, &argv );\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    \n",
    "  MPI_Type_vector(array_size, 1, size * array_size, MPI_INT, &arraytype);\n",
    "  MPI_Type_commit(&arraytype);\n",
    "\n",
    "  /* initialize array */\n",
    "  for (i=0; i < array_size; i++) {\n",
    "    myarray[i] = rank;\n",
    "  }\n",
    "\n",
    "  /* open file */\n",
    "  mode = MPI_MODE_CREATE | MPI_MODE_WRONLY;\n",
    "  MPI_File_open(MPI_COMM_WORLD, filename, mode, MPI_INFO_NULL, &outfile);\n",
    "\n",
    "  /* set file view */\n",
    "  MPI_File_set_view(outfile, rank * sizeof(MPI_INT), MPI_INT, arraytype, \"native\", MPI_INFO_NULL);\n",
    "\n",
    "  /*  write buffer to file*/\n",
    "  MPI_File_write(outfile, &myarray[0], array_size, MPI_INT, &status);    \n",
    "\n",
    "  /* print out number of bytes written */\n",
    "  MPI_Get_elements(&status, MPI_CHAR, &nbytes);\n",
    "  printf(\"TASK %d wrote %d bytes\\n\", rank, nbytes);\n",
    "\n",
    "  /* close file */\n",
    "  MPI_File_close( &outfile );\n",
    "    \n",
    "  /* finalize MPI */\n",
    "  MPI_Finalize();\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 0 wrote 16 bytes\n",
      "TASK 1 wrote 16 bytes\n",
      "TASK 2 wrote 16 bytes\n",
      "TASK 3 wrote 16 bytes\n",
      "00000000  00 00 00 00 00 00 00 00  01 00 00 00 00 00 00 00  |................|\n",
      "00000010  02 00 00 00 00 00 00 00  03 00 00 00 00 00 00 00  |................|\n",
      "00000020  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n",
      "*\n",
      "00000040  00 00 00 00 00 00 00 00  01 00 00 00 00 00 00 00  |................|\n",
      "00000050  02 00 00 00 00 00 00 00  03 00 00 00 00 00 00 00  |................|\n",
      "00000060  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n",
      "*\n",
      "00000080  00 00 00 00 00 00 00 00  01 00 00 00 00 00 00 00  |................|\n",
      "00000090  02 00 00 00 00 00 00 00  03 00 00 00 00 00 00 00  |................|\n",
      "000000a0  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n",
      "*\n",
      "000000c0  00 00 00 00 00 00 00 00  01 00 00 00 00 00 00 00  |................|\n",
      "000000d0  02 00 00 00 00 00 00 00  03 00 00 00              |............|\n",
      "000000dc\n"
     ]
    }
   ],
   "source": [
    "!mpicc codes/openmpi/mpiio_cirwrite.c -o ~/mpiio_cirwrite\n",
    "!rm output.dat\n",
    "!mpirun -np 4 --map-by core:OVERSUBSCRIBE ~/mpiio_cirwrite\n",
    "!hexdump -C output.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/openmpi/mpiio_cirread.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/openmpi/mpiio_cirread.c\n",
    "\n",
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "#define array_size 4\n",
    "\n",
    "static char filename[] = \"output.dat\"; \n",
    "\n",
    "main(int argc, char **argv)\n",
    "{\n",
    "  int rank, size;\n",
    "  MPI_File infile;\n",
    "  MPI_Status status;\n",
    "  MPI_Datatype arraytype;\n",
    "\n",
    "  int nbytes, myarray[array_size], mode, i;\n",
    "  double start, stop;\n",
    "\n",
    "  /* initialize MPI */\n",
    "  MPI_Init( &argc, &argv );\n",
    "  MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n",
    "  MPI_Comm_size( MPI_COMM_WORLD, &size );\n",
    "\n",
    "  MPI_Type_vector(array_size, 1, size*array_size, MPI_INT, &arraytype);\n",
    "  MPI_Type_commit(&arraytype);\n",
    "    \n",
    "  /* open file */\n",
    "  mode = MPI_MODE_RDONLY;\n",
    "\n",
    "  MPI_File_open( MPI_COMM_WORLD, filename, mode, MPI_INFO_NULL, &infile );\n",
    "\n",
    "  /* set file view */\n",
    "  MPI_File_set_view( infile, rank*sizeof(MPI_INT), MPI_INT, arraytype, \"native\", MPI_INFO_NULL );\n",
    "\n",
    "  /*  read file */\n",
    "  MPI_File_read( infile, &myarray[0], array_size, MPI_INT, &status );\n",
    "\n",
    "\n",
    "  /* close file */\n",
    "  MPI_File_close( &infile );\n",
    "\n",
    "  /* print out results */\n",
    "  for (i=0; i < array_size; i++) \n",
    "    printf(\"%2d%c\", myarray[i], i%4==3 ? '\\n' : ' ');\n",
    "\n",
    "  /* finalize MPI */\n",
    "  MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  0  0  0\r\n",
      " 1  1  1  1\r\n",
      " 2  2  2  2\r\n",
      " 3  3  3  3\r\n"
     ]
    }
   ],
   "source": [
    "!mpicc codes/openmpi/mpiio_cirread.c -o ~/mpiio_cirread\n",
    "!mpirun -np 4 --map-by core:OVERSUBSCRIBE ~/mpiio_cirread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/openmpi/mpiio_multiples.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/openmpi/mpiio_multiples.c\n",
    "/*  \n",
    " *  (C) 2001 by Argonne National Laboratory.\n",
    " *      See COPYRIGHT in top-level directory.\n",
    " */\n",
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "#include <string.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define SIZE (6553600)\n",
    "\n",
    "/* Each process writes to separate files and reads them back. \n",
    "   The file name is taken as a command-line argument, and the process rank \n",
    "   is appended to it. */ \n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  int *buf, i, rank, nints, len;\n",
    "  char *filename, *tmp;\n",
    "  int  errs = 0, toterrs;\n",
    "  MPI_File fh;\n",
    "  MPI_Status status;\n",
    "\n",
    "  MPI_Init(&argc,&argv);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "  /* Process 0 takes the file name as a command-line argument and \n",
    "     broadcasts it to other processes */\n",
    "  if (!rank) {\n",
    "    i = 1;\n",
    "    while ((i < argc) && strcmp(\"-fname\", *argv)) {\n",
    "      i++;\n",
    "      argv++;\n",
    "    }\n",
    "    if (i >= argc) {\n",
    "      fprintf(stderr, \"\\n*#  Usage: simple -fname filename\\n\\n\");\n",
    "      MPI_Abort(MPI_COMM_WORLD, 1);\n",
    "    }\n",
    "    argv++;\n",
    "    len = strlen(*argv);\n",
    "    filename = (char *) malloc(len+10);\n",
    "    strcpy(filename, *argv);\n",
    "    MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    MPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n",
    "  } else {\n",
    "    MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    filename = (char *) malloc(len+10);\n",
    "    MPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n",
    "  }\n",
    "\n",
    "  buf = (int *) malloc(SIZE);\n",
    "  nints = SIZE/sizeof(int);\n",
    "  for (i=0; i<nints; i++){\n",
    "    buf[i] = rank*100000 + i;\n",
    "  }\n",
    "\n",
    "  /* each process opens a separate file called filename.'myrank' */\n",
    "  tmp = (char *) malloc(len+10);\n",
    "  strcpy(tmp, filename);\n",
    "  sprintf(filename, \"%s.%d\", tmp, rank);\n",
    "\n",
    "  MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_CREATE | MPI_MODE_RDWR,MPI_INFO_NULL, &fh);\n",
    "  MPI_File_write(fh, buf, nints, MPI_INT, &status);\n",
    "  MPI_File_close(&fh);\n",
    "\n",
    "  /* reopen the file and read the data back */\n",
    "  for (i=0; i<nints; i++){\n",
    "    buf[i] = 0;\n",
    "  }\n",
    "    \n",
    "  MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n",
    "  MPI_File_read(fh, buf, nints, MPI_INT, &status);\n",
    "  MPI_File_close(&fh);\n",
    "\n",
    "  /* check if the data read is correct */\n",
    "  for (i=0; i<nints; i++) {\n",
    "    if (buf[i] != (rank*100000 + i)) {\n",
    "      errs++;\n",
    "      fprintf(stderr, \"Process %d: error, read %d, should be %d\\n\", rank, buf[i], rank*100000+i);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  MPI_Allreduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n",
    "  if (rank == 0) {\n",
    "    if( toterrs > 0) {\n",
    "      fprintf( stderr, \"Found %d errors\\n\", toterrs );\n",
    "    }\n",
    "    else {\n",
    "      fprintf( stdout, \" No Errors\\n\" );\n",
    "    }\n",
    "  }\n",
    "\n",
    "  free(buf);\n",
    "  free(filename);\n",
    "  free(tmp);\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0; \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No Errors\r\n"
     ]
    }
   ],
   "source": [
    "!mpicc codes/openmpi/mpiio_multiples.c -o ~/mpiio_multiples\n",
    "!mpirun -np 8 --map-by core:OVERSUBSCRIBE ~/mpiio_multiples -fname testmpiio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Login to Bridges Supercomputer (PSC)\n",
    "\n",
    "In a terminal, run the following command\n",
    "\n",
    "```\n",
    "$ ssh -p 2222 <Your_XSEDE_Username>@bridges.psc.edu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How are the file systems set up?\n",
    "\n",
    "```\n",
    "$ df -h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  Where are all the cool programs?\n",
    "\n",
    "```\n",
    "$ module avail\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Where are they physically?\n",
    "\n",
    "```\n",
    "$ ls /opt/packages/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> DO NOT RUN JOBS ON THE HEAD NODE </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> DO NOT RUN JOBS ON THE HEAD NODE </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What are the types of resources available to users?\n",
    "\n",
    "A \"Bridges regular memory\" allocation allows you to use Bridges' RSM (128GB) nodes. Partitions available to \"Bridges regular memory\" allocations are\n",
    "- **RM**: for jobs that will run on Bridges' RSM (128GB) nodes, and use one or more full nodes\n",
    "- **RM-shared**: for jobs that will run on Bridges' RSM (128GB) nodes, but share a node with other jobs\n",
    "- **RM-small**: for short jobs needing 2 full nodes or less, that will run on Bridges RSM (128GB) nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What are the types of resources available to users?\n",
    "\n",
    "A \"Bridges GPU\" allocation allows you to use Bridges' GPU nodes. Partitions available to \"Bridges GPU\" allocations are:\n",
    "\n",
    "- **GPU**: for jobs that will run on Bridges' GPU nodes, and use one or more full nodes\n",
    "- **GPU-shared**: for jobs that will run on Bridges' GPU nodes, but share a node with other jobs\n",
    "- **GPU-small**: for jobs that will use only one of Bridges' GPU nodes and 8 hours or less of wall time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What are the types of resources available to users?\n",
    "\n",
    "A \"Bridges large memory\" allocation allows you to use  Bridges LSM and ESM (3TB and 12TB) nodes. There is one partition available to \"Bridges large memory\" allocations:\n",
    "- **LM**: for jobs that will run on Bridges' LSM and ESM (3TB and 12TB) nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Start using Bridges\n",
    "\n",
    "```\n",
    "$ intertact -p RM -N 2 --ntasks-per-node=28 -t 60:00\n",
    "$ cd $SCRATCH\n",
    "$ nano mpiio_bigwrite.c\n",
    "$ module load mpi/gcc_openmpi\n",
    "$ module list\n",
    "$ nano mpiio_bigwrite.c\n",
    "$ COPY CONTENT FROM THE NEXT SLIDE INTO THIS FILE\n",
    "$ mpicc mpiio_bigwrite.c\n",
    "$ mpirun -np 2 ./a.out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define array_size 512000000\n",
    "\n",
    "static char filename[] = \"output.dat\";\n",
    "\n",
    "main(int argc, char **argv)\n",
    "{\n",
    "  int rank, size, offset, local_size;\n",
    "  MPI_File outfile;\n",
    "  MPI_Status status;\n",
    "  int nbytes, mode, i;\n",
    "  int *myarray;\n",
    "\n",
    "  /* initialize MPI */\n",
    "  MPI_Init( &argc, &argv );\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "  /* initialize array */\n",
    "  local_size = array_size / size;\n",
    "  myarray = malloc(local_size * sizeof(int));\n",
    "\n",
    "  for (i=0; i < local_size; i++) {\n",
    "    myarray[i] = rank;\n",
    "  }\n",
    "\n",
    "  /* open file */\n",
    "  mode = MPI_MODE_CREATE | MPI_MODE_WRONLY;\n",
    "  MPI_File_open(MPI_COMM_WORLD, filename, mode, MPI_INFO_NULL, &outfile);\n",
    "\n",
    "  offset = rank * (array_size / size) * sizeof(int);\n",
    "\n",
    "  /*  write buffer to file*/\n",
    "  MPI_File_write_at(outfile, offset, myarray, local_size, MPI_INT, &status);\n",
    "\n",
    "  /* print out number of bytes written */\n",
    "  MPI_Get_elements(&status, MPI_CHAR, &nbytes);\n",
    "  printf(\"TASK %d wrote %d bytes\\n\", rank, nbytes);\n",
    "\n",
    "  /* close file */\n",
    "  MPI_File_close( &outfile );\n",
    "\n",
    "  MPI_Barrier(MPI_COMM_WORLD);\n",
    "\n",
    "\n",
    "\n",
    "  /* finalize MPI */\n",
    "  MPI_Finalize();\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- Increase the number of processes in your mpirun (4, 8, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 54)\n",
    "- What do you observe?\n",
    "- Edit the C code and replace the path to the output file to be `$HOME/output.dat`\n",
    "- Recomlie and rerun the code with similar number of processes\n",
    "- What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/mpiio_bigwrite.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/mpiio_bigwrite.py\n",
    "#!/usr/bin/env python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "    \n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "amode = MPI.MODE_WRONLY|MPI.MODE_CREATE\n",
    "comm = MPI.COMM_WORLD\n",
    "fh = MPI.File.Open(comm, \"/scratch1/lngo/datafile.contig\", amode)\n",
    "\n",
    "local_count = (int)(1600000000 / size)\n",
    "\n",
    "buffer = np.empty(local_count, dtype=np.int)\n",
    "buffer[:] = rank\n",
    "\n",
    "offset = comm.Get_rank()*buffer.nbytes\n",
    "fh.Write_at_all(offset, buffer)  \n",
    "fh.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Running on PVFS\n",
    "```\n",
    "!chmod 755 codes/mpi4py/mpiio_bigwrite.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3;time mpirun -np 8 --mca mpi_cuda_support 0 codes/mpi4py/mpiio_bigwrite.py\n",
    "!ls -lh /scratch1/lngo/datafile.contig; rm /scratch1/lngo/datafile.contig\n",
    "!module load gcc/5.3.0 openmpi/1.10.3;time mpirun -np 16 --mca mpi_cuda_support 0 codes/mpi4py/mpiio_bigwrite.py\n",
    "!ls -lh /scratch1/lngo/datafile.contig; rm /scratch1/lngo/datafile.contig\n",
    "!module load gcc/5.3.0 openmpi/1.10.3;time mpirun -np 32 --mca mpi_cuda_support 0 codes/mpi4py/mpiio_bigwrite.py\n",
    "!ls -lh /scratch1/lngo/datafile.contig; rm /scratch1/lngo/datafile.contig\n",
    "\n",
    "real\t1m36.914s\n",
    "user\t0m4.480s\n",
    "sys\t0m25.080s\n",
    "-rw-r--r-- 1 lngo bigdata 12G Oct 23 11:35 /scratch1/lngo/datafile.contig\n",
    "\n",
    "real\t0m13.298s\n",
    "user\t0m15.483s\n",
    "sys\t0m28.579s\n",
    "-rw-r--r-- 1 lngo bigdata 12G Oct 23 11:40 /scratch1/lngo/datafile.contig\n",
    "\n",
    "real\t0m9.252s\n",
    "user\t0m21.329s\n",
    "sys\t0m14.430s\n",
    "-rw-r--r-- 1 lngo bigdata 12G Oct 23 11:40 /scratch1/lngo/datafile.contig\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/mpiio_bigwrite_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/mpiio_bigwrite_2.py\n",
    "#!/usr/bin/env python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "    \n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "amode = MPI.MODE_WRONLY|MPI.MODE_CREATE\n",
    "comm = MPI.COMM_WORLD\n",
    "fh = MPI.File.Open(comm, \"/home/lngo/datafile.contig\", amode)\n",
    "\n",
    "local_count = (int)(1600000000 / size)\n",
    "\n",
    "buffer = np.empty(local_count, dtype=np.int)\n",
    "buffer[:] = rank\n",
    "\n",
    "offset = comm.Get_rank()*buffer.nbytes\n",
    "fh.Write_at_all(offset, buffer)  \n",
    "fh.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Running on NFS\n",
    "\n",
    "```\n",
    "!chmod 755 codes/mpi4py/mpiio_bigwrite_2.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3;time mpirun -np 8 --mca mpi_cuda_support 0 codes/mpi4py/mpiio_bigwrite_2.py\n",
    "!ls -lh /home/lngo/datafile.contig; rm /home/lngo/datafile.contig\n",
    "!module load gcc/5.3.0 openmpi/1.10.3;time mpirun -np 16 --mca mpi_cuda_support 0 codes/mpi4py/mpiio_bigwrite_2.py\n",
    "!ls -lh /home/lngo/datafile.contig; rm /home/lngo/datafile.contig\n",
    "!module load gcc/5.3.0 openmpi/1.10.3;time mpirun -np 32 --mca mpi_cuda_support 0 codes/mpi4py/mpiio_bigwrite_2.py\n",
    "!ls -lh /home/lngo/datafile.contig; rm /home/lngo/datafile.contig\n",
    "\n",
    "real\t0m27.454s\n",
    "user\t0m2.941s\n",
    "sys\t0m31.769s\n",
    "-rw-r--r-- 1 lngo bigdata 12G Oct 23 11:41 /home/lngo/datafile.contig\n",
    "\n",
    "real\t0m29.080s\n",
    "user\t0m58.651s\n",
    "sys\t0m24.621s\n",
    "-rw-r--r-- 1 lngo bigdata 12G Oct 23 11:41 /home/lngo/datafile.contig\n",
    "\n",
    "real\t0m28.408s\n",
    "user\t1m45.810s\n",
    "sys\t0m17.878s\n",
    "-rw-r--r-- 1 lngo bigdata 12G Oct 23 11:42 /home/lngo/datafile.contig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Under the Covers of MPI-IO </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- MPI-IO implementation is given a lot of information in this case:\n",
    "    - Collection of processes reading data\n",
    "    - Structured description of the regions\n",
    "- Implementation has some options for how to obtain this data\n",
    "    - Noncontiguous data access optimizations\n",
    "    - Collective I/O optimizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> General Guidelines for Achieving High I/O Performance </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Buy sufficient I/O hardware for the machine\n",
    "- Use fast file systems, not NFS-mounted home directories\n",
    "- Do not perform I/O from one process only\n",
    "- Make large requests wherever possible\n",
    "- For noncontiguous requests, use derived datatypes and a single collective I/O call\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
