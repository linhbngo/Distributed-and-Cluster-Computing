{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Partitioning: Divide and Conquer</center>\n",
    "### <center> Linh B. Ngo </center>\n",
    "### <center> CPSC 4770-6770 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Partitioning </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Partitioning simply divides the problem into parts and then compute the parts and combine results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The basis of all parallel programming, in one form or another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Pleasantly parallel used partitioning without any interaction between the parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Most partitioning  formulation require the results of the parts to be combined to obtain the desired results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Partitioning can be applied to the program data. This is call data partitioning or domain decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Partitioning can also be applied to the functions of a program. This is called functional decomposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Divide and Conquer </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Characterized by dividing problem into sub-problems of same form as larger problem. Further divisions into still smaller sub-problems, usually done by recursion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recursive divide and conquer amenable to parallelization because separate processes can be used for divided pairs. Also usually data is naturally localized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/dc01.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/dc02.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/divide.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/divide.py\n",
    "#!/usr/bin/env python\n",
    "# divide.py\n",
    "from mpi4py import MPI\n",
    "import math\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank(); size = comm.Get_size()\n",
    "for i in range(0, int(math.log2(size))):\n",
    "    distance = int(math.pow(2, i))\n",
    "    if ( rank < int(math.pow(2, i)) ):\n",
    "        print (\"Iteration \", i, \": sender \", rank, \n",
    "               \" sends to \", rank + distance)\n",
    "    if (rank >= int(math.pow(2, i)) and rank < int(math.pow(2,i+1))):\n",
    "        print (\"Iteration \", i, \": receiver \", rank, \n",
    "               \"receives from \", rank - distance)\n",
    "    #am I sender or receiver?\n",
    "    # who am I sending/receiving to/from?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : sender  0  sends to  1\r\n",
      "Iteration  1 : sender  0  sends to  2\r\n",
      "Iteration  2 : sender  0  sends to  4\r\n",
      "Iteration  0 : receiver  1 receives from  0\r\n",
      "Iteration  1 : sender  1  sends to  3\r\n",
      "Iteration  2 : sender  1  sends to  5\r\n",
      "Iteration  1 : receiver  2 receives from  0\r\n",
      "Iteration  2 : sender  2  sends to  6\r\n",
      "Iteration  1 : receiver  3 receives from  1\r\n",
      "Iteration  2 : sender  3  sends to  7\r\n",
      "Iteration  2 : receiver  4 receives from  0\r\n",
      "Iteration  2 : receiver  5 receives from  1\r\n",
      "Iteration  2 : receiver  6 receives from  2\r\n",
      "Iteration  2 : receiver  7 receives from  3\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/divide.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 8 codes/mpi4py/divide.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/dc03.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/conquer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/conquer.py\n",
    "#!/usr/bin/env python\n",
    "# conquer.py\n",
    "from mpi4py import MPI\n",
    "import math\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank(); size = comm.Get_size()\n",
    "for i in range(int(math.log2(size)) - 1, -1, -1):\n",
    "    distance = int(math.pow(2, i))\n",
    "    if ( rank < int(math.pow(2, i)) ):\n",
    "        print (\"Iteration \", i, \": receiver \", rank, \n",
    "               \" receives from \", rank + distance)\n",
    "    if (rank >= int(math.pow(2, i)) and rank < int(math.pow(2,i+1))):\n",
    "        print (\"Iteration \", i, \": sender \", rank, \n",
    "               \"sends to \", rank - distance)\n",
    "    #am I sender or receiver?\n",
    "    # who am I sending/receiving to/from?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  2 : sender  5 sends to  1\r\n",
      "Iteration  2 : sender  6 sends to  2\r\n",
      "Iteration  2 : sender  7 sends to  3\r\n",
      "Iteration  2 : receiver  0  receives from  4\r\n",
      "Iteration  1 : receiver  0  receives from  2\r\n",
      "Iteration  0 : receiver  0  receives from  1\r\n",
      "Iteration  2 : receiver  1  receives from  5\r\n",
      "Iteration  1 : receiver  1  receives from  3\r\n",
      "Iteration  0 : sender  1 sends to  0\r\n",
      "Iteration  2 : receiver  2  receives from  6\r\n",
      "Iteration  1 : sender  2 sends to  0\r\n",
      "Iteration  2 : receiver  3  receives from  7\r\n",
      "Iteration  1 : sender  3 sends to  1\r\n",
      "Iteration  2 : sender  4 sends to  0\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/conquer.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 8 codes/mpi4py/conquer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Many sorting algorithms can be parallelized by partitioning using\n",
    "divide and conquer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Bucket Sort </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/bucketsort1.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Simple approach to parallel bucket sort **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/bucketsort2.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Bcast data\n",
    "- Sort only those elements that fit in local interval bucket (determined by rank)\n",
    "- Gather sorted bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Uppercase collective: MPI.Scatterv**\n",
    "\n",
    "Comm.Scatterv([sendbuf, tuple_int sendcounts, tuple_int displacements, MPI_Datatype sendtype], recvbuf, root=0)\n",
    "\n",
    "Parameters:\t\n",
    "- Comm (MPI comm) – communicator across which to scatter\n",
    "- sendbuf (choice) – buffer\n",
    "- sendcounts (tuple_int) – number of elements to send to each process (one integer for each process)\n",
    "- displacements (tuple_int) – number of elements away from the first element in the array at which to begin the new, segmented array\n",
    "- sendtype (MPI_Datatype) – MPI datatype of the buffer being sent (choice of sendbuf)\n",
    "- recvbuf (choice) – buffer in which to receive the sendbuf\n",
    "- root (int) – process from which to scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/scatterv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/scatterv.py\n",
    "#!/usr/bin/env python\n",
    "# scatterv.py\n",
    "# Run with 3 processes\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD; rank = comm.Get_rank()\n",
    "if rank == 0:\n",
    "    x = numpy.linspace(0,100,11)\n",
    "    print (x)\n",
    "else:\n",
    "    x = None\n",
    "if rank == 2:\n",
    "    xlocal = numpy.zeros(9)\n",
    "else:\n",
    "    xlocal = numpy.zeros(1)\n",
    "comm.Scatterv([x,(1,1,9),(0,1,2),MPI.DOUBLE],xlocal)\n",
    "print (\"process \" + str(rank) + \" has \" +str(xlocal))\n",
    "#if (rank == 0):\n",
    "  #  print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "The library attempted to open the following supporting CUDA libraries, \n",
      "but each of them failed.  CUDA-aware support is disabled.\n",
      "libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "libcuda.dylib: cannot open shared object file: No such file or directory\n",
      "/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory\n",
      "If you are not interested in CUDA-aware support, then run with \n",
      "--mca mpi_cuda_support 0 to suppress this message.  If you are interested\n",
      "in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location\n",
      "of libcuda.so.1 to get passed this issue.\n",
      "--------------------------------------------------------------------------\n",
      "process 1 has [ 10.]\n",
      "process 2 has [  20.   30.   40.   50.   60.   70.   80.   90.  100.]\n",
      "[   0.   10.   20.   30.   40.   50.   60.   70.   80.   90.  100.]\n",
      "process 0 has [ 0.]\n",
      "[node0018.palmetto.clemson.edu:09396] 2 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed\n",
      "[node0018.palmetto.clemson.edu:09396] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/scatterv.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 3 codes/mpi4py/scatterv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Uppercase collective: MPI.Gatherv**\n",
    "\n",
    "Comm.Gatherv(sendbuf, [recvbuf, tuple_int sendcounts, tuple_int displacements, MPI_Datatype sendtype], root=0)\n",
    "\n",
    "Parameters:\t\n",
    "- Comm (MPI comm) – communicator across which to scatter\n",
    "- sendbuf (choice) – buffer\n",
    "- recvbuf (choice) – buffer in which to receive the sendbuf\n",
    "- sendcounts (tuple_int) – number of elements to receive from each process (one integer for each process)\n",
    "- displacements (tuple_int) – number of elements away from the first element in the receiving array at which to begin appending the segmented array\n",
    "- sendtype (MPI_Datatype) – MPI datatype of the buffer being sent (choice of sendbuf)\n",
    "- root (int) – process from which to scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/gatherv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/gatherv.py\n",
    "#!/usr/bin/env python\n",
    "# gatherv.py\n",
    "# Run with 3 processes\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD; rank = comm.Get_rank()\n",
    "if rank == 0:\n",
    "    x = numpy.linspace(0,80,9)\n",
    "    print (x)\n",
    "else:\n",
    "    x = None\n",
    "if rank == 1:\n",
    "    xlocal = numpy.zeros(7)\n",
    "else:\n",
    "    xlocal = numpy.zeros(1)\n",
    "comm.Scatterv([x,(1,7,1),(0,1,8),MPI.DOUBLE],xlocal); \n",
    "if rank == 0:\n",
    "    xGathered = numpy.zeros(9)\n",
    "else:\n",
    "    xGathered = None\n",
    "comm.Gatherv(xlocal,[xGathered,(1,7,1),(0,1,1),MPI.DOUBLE])\n",
    "print (\" process \", rank, \" has \", xlocal); \n",
    "if (rank == 0):\n",
    "    print (\"process \" + str(rank) + \" has \" +str(xGathered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.  10.  20.  30.  40.  50.  60.  70.  80.]\r\n",
      " process  0  has  [ 0.]\r\n",
      "process 0 has [  0.  80.  20.  30.  40.  50.  60.  70.   0.]\r\n",
      " process  1  has  [ 10.  20.  30.  40.  50.  60.  70.]\r\n",
      " process  2  has  [ 80.]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/gatherv.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 3 --mca mpi_cuda_support 0 codes/mpi4py/gatherv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Parallel Bucket Sort version 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/bucket1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/bucket1.py\n",
    "#!/usr/bin/env python\n",
    "# bucket1.py\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "rank = comm.Get_rank(); size = comm.Get_size(); N = 16\n",
    "\n",
    "unsorted = np.zeros(N, dtype=\"int\")\n",
    "final_sorted = np.zeros(N, dtype=\"int\")\n",
    "\n",
    "if rank == 0:\n",
    "    unsorted = np.random.randint(low=0,high=N,size=N)\n",
    "    \n",
    "comm.Bcast(unsorted)\n",
    "local_min = rank * N / size\n",
    "local_max = (rank + 1) * N / size\n",
    "\n",
    "#generic form:\n",
    "# local_min = rank * (b - a) / size\n",
    "# local_max = (rank + 1) * (b - a) / size\n",
    "\n",
    "print(\"At rank \", rank, \": \", local_min, local_max)\n",
    "\n",
    "local_bucket = unsorted[np.logical_and(unsorted >= local_min, \n",
    "                                       unsorted < local_max)]\n",
    "local_sorted = np.sort(local_bucket)\n",
    "\n",
    "arr_sendcount = np.zeros(size, dtype=\"int\")\n",
    "sendcount = np.array([local_sorted.size], dtype=\"int\")\n",
    "\n",
    "comm.Gather(sendcount, arr_sendcount, root=0)\n",
    "comm.Bcast(arr_sendcount, root = 0)\n",
    "\n",
    "dispcount = np.zeros(size, dtype=\"int\")\n",
    "dispcount[0] = 0\n",
    "for i in range(1,size):\n",
    "    dispcount[i] = dispcount[i-1] + arr_sendcount[i-1]\n",
    "    \n",
    "comm.Gatherv(local_sorted, [final_sorted, \n",
    "                            tuple(arr_sendcount), \n",
    "                            tuple(dispcount), \n",
    "                            MPI.DOUBLE])\n",
    "\n",
    "if (rank == 0):\n",
    "    print (\"At root: \", unsorted)\n",
    "    print (\"At root: \", final_sorted)\n",
    "print(\"Rank\", rank, \": \", local_bucket)\n",
    "print(\"Rank\", rank, \": \", local_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At rank  0 :  0.0 4.0\r\n",
      "At root:  [15  8  5  1 10 11 15 10  7 12 11  4  1 11 12 11]\r\n",
      "At root:  [ 1  1  4  5  7  8 10 10 11 11 11 11 12 12 15 15]\r\n",
      "Rank 0 :  [1 1]\r\n",
      "Rank 0 :  [1 1]\r\n",
      "At rank  1 :  4.0 8.0\r\n",
      "Rank 1 :  [5 7 4]\r\n",
      "At rank  2 :  8.0 12.0\r\n",
      "Rank 2 :  [ 8 10 11 10 11 11 11]\r\n",
      "Rank 2 :  [ 8 10 10 11 11 11 11]\r\n",
      "At rank  3 :  12.0 16.0\r\n",
      "Rank 3 :  [15 15 12 12]\r\n",
      "Rank 3 :  [12 12 15 15]\r\n",
      "Rank 1 :  [4 5 7]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/bucket1.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 --mca mpi_cuda_support 0  codes/mpi4py/bucket1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/bucketsort3.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/all2all.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/all2all_2.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Uppercase collective: MPI.Alltoallv**\n",
    "\n",
    "Comm.Alltoallv([sendbuf, tuple_int sendcounts, tuple_int displacements, MPI_Datatype sendtype],[recvbuf, tuple_int recvcounts, tuple_int displacements, MPI_Datatype sendtype])\n",
    "\n",
    "Parameters:\t\n",
    "- Comm (MPI comm) – communicator across which to scatter\n",
    "- sendbuf (choice) – buffer\n",
    "- recvbuf (choice) – buffer in which to receive the sendbuf\n",
    "- sendcounts (tuple_int) – number of elements to send to each process (one integer for each process)\n",
    "- displacements (tuple_int) – number of elements away from the first element in the sending array at which to begin sending the segmented array\n",
    "- sendtype (MPI_Datatype) – MPI datatype of the buffer being sent (choice of sendbuf)\n",
    "- recvcounts (tuple_int) – number of elements to receive from each process (one integer for each process)\n",
    "- displacements (tuple_int) – number of elements away from the first element in the receiving array at which to begin appending the segmented array\n",
    "- sendtype (MPI_Datatype) – MPI datatype of the buffer being receive (choice of recvbuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/all2allv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/all2allv.py\n",
    "#!/usr/bin/env python\n",
    "# all2allv.py\n",
    "import numpy\n",
    "import random\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank(); size = comm.Get_size(); N = 8\n",
    "local_array = numpy.random.randint(low = 0, high = N, size = N)\n",
    "print(\"Rank \", rank, \"local_array: \", local_array)\n",
    "#stackoverflow\n",
    "def constrained_sum_sample_pos(n, total):\n",
    "    dividers = sorted(random.sample(range(1, total), n - 1))\n",
    "    return [a - b for a, b in zip(dividers + [total], [0] + dividers)]\n",
    "sendcount = numpy.array(constrained_sum_sample_pos(size, N), dtype=\"int\")\n",
    "dispcount = numpy.zeros(size, dtype=\"int\")\n",
    "dispcount[0] = 0\n",
    "for i in range(1,size):\n",
    "    dispcount[i] = dispcount[i-1] + sendcount[i-1]    \n",
    "print (\"Rank \", rank, \" sendcount: \", sendcount)\n",
    "print (\"Rank \", rank, \" dispcount: \", dispcount)\n",
    "recvcount = numpy.zeros(size, dtype=\"int\")\n",
    "comm.Alltoall(sendcount, recvcount)\n",
    "print(\"Rank \", rank, \" recvcount: \", recvcount)\n",
    "disprecv = numpy.zeros(size, dtype=\"int\")\n",
    "disprecv[0] = 0\n",
    "for i in range(1,size):\n",
    "    disprecv[i] = disprecv[i-1] + recvcount[i-1]   \n",
    "print(\"Rank \", rank, \" disprecv: \", disprecv)\n",
    "new_array = numpy.zeros(numpy.sum(recvcount), dtype=\"int\")\n",
    "comm.Alltoallv([local_array,tuple(sendcount),tuple(dispcount),MPI.DOUBLE],\n",
    "               [new_array, tuple(recvcount), tuple(disprecv), MPI.DOUBLE])\n",
    "print (\"Rank \", rank, \"new_array: \", new_array)\n",
    "#print (local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  0 local_array:  [4 0 2 1 2 4 5 5]\r\n",
      "Rank  0  sendcount:  [1 1 4 2]\r\n",
      "Rank  0  dispcount:  [0 1 2 6]\r\n",
      "Rank  1 local_array:  [4 2 7 4 5 1 6 0]\r\n",
      "Rank  1  sendcount:  [3 1 1 3]\r\n",
      "Rank  1  dispcount:  [0 3 4 5]\r\n",
      "Rank  2 local_array:  [5 0 2 4 6 6 0 4]\r\n",
      "Rank  2  sendcount:  [1 1 2 4]\r\n",
      "Rank  2  dispcount:  [0 1 2 4]\r\n",
      "Rank  3 local_array:  [4 1 3 4 7 2 5 0]\r\n",
      "Rank  3  sendcount:  [2 1 4 1]\r\n",
      "Rank  3  dispcount:  [0 2 3 7]\r\n",
      "Rank  2  recvcount:  [4 1 2 4]\r\n",
      "Rank  2  disprecv:  [0 4 5 7]\r\n",
      "Rank  2 new_array:  [2 1 2 4 5 2 4 4 7 2 5]\r\n",
      "Rank  3  recvcount:  [2 3 4 1]\r\n",
      "Rank  3  disprecv:  [0 2 5 9]\r\n",
      "Rank  3 new_array:  [5 5 1 6 0 6 6 0 4 0]\r\n",
      "Rank  0  recvcount:  [1 3 1 2]\r\n",
      "Rank  0  disprecv:  [0 1 4 5]\r\n",
      "Rank  0 new_array:  [4 4 2 7 5 4 1]\r\n",
      "Rank  1  recvcount:  [1 1 1 1]\r\n",
      "Rank  1  disprecv:  [0 1 2 3]\r\n",
      "Rank  1 new_array:  [0 4 0 3]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/all2allv.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 --mca mpi_cuda_support 0  codes/mpi4py/all2allv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/bucket2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/bucket2.py\n",
    "#!/usr/bin/env python\n",
    "# bucket2.py\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank(); size = comm.Get_size(); N = 16\n",
    "unsorted = None\n",
    "local_unsorted = np.zeros(int(N / size), dtype=\"int\")\n",
    "final_sorted = np.zeros(N, dtype=\"int\")\n",
    "if rank == 0:\n",
    "    unsorted = np.random.randint(low=0,high=N,size=N)\n",
    "    print (\"Original unsorted data at process\", rank, \": \", unsorted)\n",
    "comm.Scatter(unsorted, local_unsorted, root = 0)\n",
    "print (\"Scattered data at process \", rank, \": \", local_unsorted)\n",
    "\n",
    "local_buckets = np.full(shape=(int(N/size), size), fill_value=-1, dtype=\"int\")\n",
    "sendcount = np.zeros(size, dtype=\"int\")\n",
    "local_unsorted_bucketed = np.zeros(int(N / size), dtype=\"int\")\n",
    "count = 0\n",
    "for i in range(0, size):    \n",
    "    for j in range (0, local_unsorted.size):\n",
    "        local_min = i * N/size\n",
    "        local_max = (i + 1) * N / size  \n",
    "        if ((local_unsorted[j] >= local_min) and (local_unsorted[j] < local_max)):\n",
    "            local_buckets[i][sendcount[i]] = local_unsorted[j]\n",
    "            sendcount[i] += 1\n",
    "    if (sendcount[i] > 0):\n",
    "        local_unsorted_bucketed[count:count + sendcount[i]] = local_buckets[i, 0:sendcount[i]]\n",
    "        print (\"Local_unsorted_bucketed at process \", rank, \": \", local_unsorted_bucketed)\n",
    "        count += sendcount[i]\n",
    "\n",
    "print (\"Sendcount at process \", rank, \": \", sendcount)\n",
    "print (\"Bucket matrix at process \", rank, \": \\n\", local_buckets)\n",
    "\n",
    "dispcount = np.zeros(size, dtype=\"int\")\n",
    "dispcount[0] = 0\n",
    "for i in range(1,size):\n",
    "    dispcount[i] = dispcount[i-1] + sendcount[i-1]    \n",
    "\n",
    "recvcount = np.zeros(size, dtype=\"int\")\n",
    "comm.Alltoall(sendcount, recvcount)\n",
    "print(\"Rank \", rank, \" recvcount: \", recvcount)\n",
    "disprecv = np.zeros(size, dtype=\"int\")\n",
    "disprecv[0] = 0\n",
    "for i in range(1,size):\n",
    "    disprecv[i] = disprecv[i-1] + recvcount[i-1]   \n",
    "print(\"Rank \", rank, \" disprecv: \", disprecv)\n",
    "single_unsorted_bucket = np.zeros(np.sum(recvcount), dtype=\"int\")\n",
    "comm.Alltoallv([local_unsorted_bucketed,tuple(sendcount),tuple(dispcount),MPI.DOUBLE],\n",
    "               [single_unsorted_bucket, tuple(recvcount), tuple(disprecv), MPI.DOUBLE])\n",
    "print (\"Rank \", rank, \" single_unsorted_bucket: \", single_unsorted_bucket)\n",
    "local_sorted = np.sort(single_unsorted_bucket)\n",
    "arr_sendcount = np.zeros(size, dtype=\"int\")\n",
    "sendcount = np.array([local_sorted.size], dtype=\"int\")\n",
    "comm.Gather(sendcount, arr_sendcount, root=0)\n",
    "comm.Bcast(arr_sendcount, root = 0)\n",
    "dispcount = np.zeros(size, dtype=\"int\")\n",
    "dispcount[0] = 0\n",
    "for i in range(1,size):\n",
    "    dispcount[i] = dispcount[i-1] + arr_sendcount[i-1]\n",
    "comm.Gatherv(local_sorted, [final_sorted, tuple(arr_sendcount), tuple(dispcount), MPI.DOUBLE])\n",
    "if (rank == 0):\n",
    "    print (final_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original unsorted data at process 0 :  [ 3 11 14  7 15 15 12 13 12  8  1  7  5 11 13 14]\r\n",
      "Scattered data at process  0 :  [ 3 11 14  7]\r\n",
      "Local_unsorted_bucketed at process  0 :  [3 0 0 0]\r\n",
      "Local_unsorted_bucketed at process  0 :  [3 7 0 0]\r\n",
      "Scattered data at process  1 :  [15 15 12 13]\r\n",
      "Local_unsorted_bucketed at process  1 :  [15 15 12 13]\r\n",
      "Scattered data at process  2 :  [12  8  1  7]\r\n",
      "Local_unsorted_bucketed at process  2 :  [1 0 0 0]\r\n",
      "Local_unsorted_bucketed at process  2 :  [1 7 0 0]\r\n",
      "Scattered data at process  3 :  [ 5 11 13 14]\r\n",
      "Local_unsorted_bucketed at process  3 :  [5 0 0 0]\r\n",
      "Local_unsorted_bucketed at process  3 :  [ 5 11  0  0]\r\n",
      "Local_unsorted_bucketed at process  3 :  [ 5 11 13 14]\r\n",
      "Sendcount at process  3 :  [0 1 1 2]\r\n",
      "Bucket matrix at process  3 : \r\n",
      " [[-1 -1 -1 -1]\r\n",
      " [ 5 -1 -1 -1]\r\n",
      " [11 -1 -1 -1]\r\n",
      " [13 14 -1 -1]]\r\n",
      "Rank  3  recvcount:  [1 4 1 2]\r\n",
      "Rank  3  disprecv:  [0 1 5 6]\r\n",
      "Local_unsorted_bucketed at process  0 :  [ 3  7 11  0]\r\n",
      "Local_unsorted_bucketed at process  0 :  [ 3  7 11 14]\r\n",
      "Sendcount at process  0 :  [1 1 1 1]\r\n",
      "Bucket matrix at process  0 : \r\n",
      " [[ 3 -1 -1 -1]\r\n",
      " [ 7 -1 -1 -1]\r\n",
      " [11 -1 -1 -1]\r\n",
      " [14 -1 -1 -1]]\r\n",
      "Rank  0  recvcount:  [1 0 1 0]\r\n",
      "Rank  0  disprecv:  [0 1 1 2]\r\n",
      "Rank  0  single_unsorted_bucket:  [3 1]\r\n",
      "[ 1  3  5  7  7  8 11 11 12 12 13 13 14 14 15 15]\r\n",
      "Sendcount at process  1 :  [0 0 0 4]\r\n",
      "Bucket matrix at process  1 : \r\n",
      " [[-1 -1 -1 -1]\r\n",
      " [-1 -1 -1 -1]\r\n",
      " [-1 -1 -1 -1]\r\n",
      " [15 15 12 13]]\r\n",
      "Rank  1  recvcount:  [1 0 1 1]\r\n",
      "Rank  1  disprecv:  [0 1 1 2]\r\n",
      "Rank  1  single_unsorted_bucket:  [7 7 5]\r\n",
      "Local_unsorted_bucketed at process  2 :  [1 7 8 0]\r\n",
      "Local_unsorted_bucketed at process  2 :  [ 1  7  8 12]\r\n",
      "Sendcount at process  2 :  [1 1 1 1]\r\n",
      "Bucket matrix at process  2 : \r\n",
      " [[ 1 -1 -1 -1]\r\n",
      " [ 7 -1 -1 -1]\r\n",
      " [ 8 -1 -1 -1]\r\n",
      " [12 -1 -1 -1]]\r\n",
      "Rank  2  recvcount:  [1 0 1 1]\r\n",
      "Rank  2  disprecv:  [0 1 1 2]\r\n",
      "Rank  2  single_unsorted_bucket:  [11  8 11]\r\n",
      "Rank  3  single_unsorted_bucket:  [14 15 15 12 13 12 13 14]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/bucket2.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 --mca mpi_cuda_support 0  codes/mpi4py/bucket2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> N-Body Problem </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Fundamental settings for most, if not all, of computational simulation problems: **\n",
    "\n",
    "- Given a space\n",
    "- Given a group of entities whose activities are (often) bounded within this space\n",
    "- Given a set of equation that governs how these entities react to one another and to attributes of the containing space\n",
    "- Simulate how these reactions impact all entities and the entire space overtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Computation requires parallelization\n",
    "- Experimental spaces are simulated at massive scale (millions of entities)\n",
    "- Individual time steps are significantly smaller than the total simulation time. \n",
    "- Time complexity can be reduced by approximating a cluster of distant bodies as a single distant body with mass sited at the center of the mass of the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/mass-bodies.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Barnes-Hut Algorithm (2-D)\n",
    "\n",
    "Start with whole region in which one square contains the bodies (or particles).\n",
    "- First, this cube is divided into four subregions.\n",
    "- If a subregion contains no particles, it is deleted from further consideration.\n",
    "- If a subregion contains one body, it is retained.\n",
    "- If a subregion contains more than one body, it is recursively divided until every subregion contains one body.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Create an quadtree – a tree with up to four edges from each node\n",
    "- The leaves represent cells each containing one body.\n",
    "- After the tree has been constructed, the total mass and center of mass of the subregion is stored at each node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/barnes-hut.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Orthogonal Recursive Bisection\n",
    "\n",
    "- First, a vertical line found that divides area into two areas each with equal number of bodies. \n",
    "- For each area, a horizontal line found that divides it into two areas, each with equal number of bodies. \n",
    "- Repeated as required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/orthogonal.png\" width=\"400\"/> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
